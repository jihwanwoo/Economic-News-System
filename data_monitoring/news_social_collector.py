#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ë° ì†Œì…œë¯¸ë””ì–´ ëª¨ë‹ˆí„°ë§ ê°•í™” ìˆ˜ì§‘ê¸° (ì‹¤ì œ Reddit API í†µí•©)
"""

import requests
import feedparser
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from textblob import TextBlob
import re
import time
import os

# Reddit ìˆ˜ì§‘ê¸° import
from data_monitoring.reddit_collector import RedditEconomicCollector

class EnhancedNewsCollector:
    """ê°•í™”ëœ ë‰´ìŠ¤ ë° ì†Œì…œë¯¸ë””ì–´ ìˆ˜ì§‘ê¸° (ì‹¤ì œ Reddit ë°ì´í„° í¬í•¨)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # Reddit ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        try:
            self.reddit_collector = RedditEconomicCollector()
            self.use_reddit = True
            self.logger.info("âœ… Reddit ìˆ˜ì§‘ê¸° í™œì„±í™”")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Reddit ìˆ˜ì§‘ê¸° ë¹„í™œì„±í™”: {e}")
            self.use_reddit = False
        
        # í™•ìž¥ëœ ë‰´ìŠ¤ ì†ŒìŠ¤
        self.news_sources = {
            "financial": [
                {"name": "Bloomberg Markets", "url": "https://feeds.bloomberg.com/markets/news.rss"},
                {"name": "Reuters Business", "url": "https://feeds.reuters.com/reuters/businessNews"},
                {"name": "MarketWatch", "url": "https://feeds.marketwatch.com/marketwatch/topstories/"},
                {"name": "CNN Money", "url": "https://rss.cnn.com/rss/money_latest.rss"},
                {"name": "Yahoo Finance", "url": "https://feeds.finance.yahoo.com/rss/2.0/headline"},
                {"name": "Financial Times", "url": "https://www.ft.com/rss/home"},
                {"name": "Wall Street Journal", "url": "https://feeds.a.dj.com/rss/RSSMarketsMain.xml"}
            ],
            "economic": [
                {"name": "Federal Reserve", "url": "https://www.federalreserve.gov/feeds/press_all.xml"},
                {"name": "Treasury", "url": "https://home.treasury.gov/rss/press-releases"},
                {"name": "BLS News", "url": "https://www.bls.gov/feed/news_release/rss.xml"},
                {"name": "Commerce Dept", "url": "https://www.commerce.gov/rss.xml"}
            ],
            "international": [
                {"name": "ECB Press", "url": "https://www.ecb.europa.eu/rss/press.xml"},
                {"name": "Bank of Japan", "url": "https://www.boj.or.jp/en/rss/whatsnew.xml"},
                {"name": "IMF News", "url": "https://www.imf.org/en/News/RSS?language=eng"}
            ]
        }
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        self.keywords = {
            "monetary_policy": ["fed", "federal reserve", "interest rate", "monetary policy", "inflation", "deflation"],
            "market_sentiment": ["bull", "bear", "rally", "crash", "volatility", "correction"],
            "economic_indicators": ["gdp", "unemployment", "cpi", "ppi", "retail sales", "housing"],
            "geopolitical": ["trade war", "sanctions", "brexit", "election", "policy", "regulation"],
            "corporate": ["earnings", "merger", "acquisition", "ipo", "bankruptcy", "dividend"],
            "technology": ["ai", "blockchain", "crypto", "fintech", "digital", "innovation"]
        }
        
        self.logger.info("âœ… ê°•í™”ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def collect_news_by_category(self, max_items_per_source: int = 10) -> Dict[str, Any]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        self.logger.info("ðŸ“° ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìž‘")
        
        news_data = {
            "timestamp": datetime.now().isoformat(),
            "categories": {},
            "summary": {}
        }
        
        total_articles = 0
        
        for category, sources in self.news_sources.items():
            category_articles = []
            
            for source in sources:
                try:
                    articles = self._fetch_rss_feed(source["url"], max_items_per_source)
                    
                    for article in articles:
                        article["source_name"] = source["name"]
                        article["category"] = category
                        
                        # ê°ì • ë¶„ì„ ì¶”ê°€
                        article["sentiment"] = self._analyze_sentiment(article["title"] + " " + article.get("summary", ""))
                        
                        # í‚¤ì›Œë“œ ë¶„ë¥˜
                        article["topics"] = self._classify_topics(article["title"] + " " + article.get("summary", ""))
                        
                        category_articles.append(article)
                    
                    time.sleep(0.5)  # Rate limiting
                    
                except Exception as e:
                    self.logger.error(f"âŒ {source['name']} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    continue
            
            news_data["categories"][category] = category_articles
            total_articles += len(category_articles)
            
            self.logger.info(f"âœ… {category}: {len(category_articles)}ê°œ ê¸°ì‚¬")
        
        # ìš”ì•½ ì •ë³´ ìƒì„±
        news_data["summary"] = self._generate_news_summary(news_data["categories"])
        news_data["summary"]["total_articles"] = total_articles
        
        self.logger.info(f"âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {total_articles}ê°œ ê¸°ì‚¬")
        return news_data
    
    def _fetch_rss_feed(self, url: str, max_items: int) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            feed = feedparser.parse(url)
            articles = []
            
            for entry in feed.entries[:max_items]:
                article = {
                    "title": entry.get("title", ""),
                    "link": entry.get("link", ""),
                    "published": entry.get("published", ""),
                    "summary": entry.get("summary", ""),
                    "author": entry.get("author", ""),
                    "published_parsed": entry.get("published_parsed", None)
                }
                
                # ë°œí–‰ ì‹œê°„ ì •ê·œí™”
                if article["published_parsed"]:
                    article["published_datetime"] = datetime(*article["published_parsed"][:6])
                else:
                    article["published_datetime"] = datetime.now()
                
                articles.append(article)
            
            return articles
            
        except Exception as e:
            self.logger.error(f"RSS í”¼ë“œ íŒŒì‹± ì˜¤ë¥˜ ({url}): {e}")
            return []
    
    def _analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"""
        try:
            blob = TextBlob(text)
            polarity = blob.sentiment.polarity
            subjectivity = blob.sentiment.subjectivity
            
            # ê°ì • ë¼ë²¨ë§
            if polarity > 0.1:
                label = "positive"
            elif polarity < -0.1:
                label = "negative"
            else:
                label = "neutral"
            
            return {
                "polarity": round(polarity, 3),
                "subjectivity": round(subjectivity, 3),
                "label": label
            }
            
        except Exception as e:
            self.logger.debug(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"polarity": 0, "subjectivity": 0, "label": "neutral"}
    
    def _classify_topics(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼ì œ ë¶„ë¥˜"""
        text_lower = text.lower()
        topics = []
        
        for topic, keywords in self.keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                topics.append(topic)
        
        return topics
    
    def _generate_news_summary(self, categories: Dict[str, List]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ìš”ì•½ ìƒì„±"""
        summary = {
            "by_category": {},
            "sentiment_analysis": {},
            "trending_topics": {},
            "recent_highlights": []
        }
        
        all_articles = []
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
        for category, articles in categories.items():
            if articles:
                sentiments = [article["sentiment"]["label"] for article in articles]
                summary["by_category"][category] = {
                    "count": len(articles),
                    "positive": sentiments.count("positive"),
                    "negative": sentiments.count("negative"),
                    "neutral": sentiments.count("neutral")
                }
                all_articles.extend(articles)
        
        # ì „ì²´ ê°ì • ë¶„ì„
        if all_articles:
            all_sentiments = [article["sentiment"]["label"] for article in all_articles]
            total_count = len(all_sentiments)
            
            summary["sentiment_analysis"] = {
                "total_articles": total_count,
                "positive": all_sentiments.count("positive"),
                "negative": all_sentiments.count("negative"),
                "neutral": all_sentiments.count("neutral"),
                "positive_ratio": round(all_sentiments.count("positive") / total_count * 100, 1),
                "negative_ratio": round(all_sentiments.count("negative") / total_count * 100, 1)
            }
            
            # ì£¼ì œë³„ íŠ¸ë Œë“œ
            topic_counts = {}
            for article in all_articles:
                for topic in article.get("topics", []):
                    topic_counts[topic] = topic_counts.get(topic, 0) + 1
            
            summary["trending_topics"] = dict(sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:10])
            
            # ìµœê·¼ ì£¼ìš” ë‰´ìŠ¤ (ê°ì • ì ìˆ˜ ê¸°ì¤€)
            recent_articles = sorted(all_articles, key=lambda x: x["published_datetime"], reverse=True)[:20]
            important_articles = sorted(recent_articles, key=lambda x: abs(x["sentiment"]["polarity"]), reverse=True)[:5]
            
            summary["recent_highlights"] = [
                {
                    "title": article["title"],
                    "source": article["source_name"],
                    "sentiment": article["sentiment"]["label"],
                    "topics": article["topics"]
                }
                for article in important_articles
            ]
        
        return summary
    
    def get_social_media_mentions(self) -> Dict[str, Any]:
        """ì†Œì…œë¯¸ë””ì–´ ì–¸ê¸‰ ë¶„ì„ (ì‹¤ì œ Reddit ë°ì´í„° + Twitter ì‹œë®¬ë ˆì´ì…˜)"""
        
        social_data = {
            "timestamp": datetime.now().isoformat(),
            "platforms": {},
            "overall_sentiment": {}
        }
        
        # Reddit ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘
        if self.use_reddit:
            try:
                self.logger.info("ðŸ“± Reddit ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                reddit_data = self.reddit_collector.collect_comprehensive_data(
                    max_subreddits=5, 
                    posts_per_subreddit=10
                )
                
                reddit_summary = reddit_data.get('summary', {})
                reddit_sentiment = reddit_data.get('sentiment_analysis', {})
                
                social_data["platforms"]["reddit"] = {
                    "posts": reddit_summary.get('total_posts', 0),
                    "comments": reddit_summary.get('total_comments', 0),
                    "total_content": reddit_summary.get('total_content', 0),
                    "avg_post_score": reddit_summary.get('avg_post_score', 0),
                    "sentiment": {
                        "positive": reddit_sentiment.get('percentages', {}).get('positive', 0),
                        "negative": reddit_sentiment.get('percentages', {}).get('negative', 0),
                        "neutral": reddit_sentiment.get('percentages', {}).get('neutral', 0)
                    },
                    "overall_sentiment": reddit_sentiment.get('overall_sentiment', 'neutral'),
                    "average_polarity": reddit_sentiment.get('average_polarity', 0),
                    "top_subreddits": list(reddit_summary.get('top_subreddits', {}).keys())[:5],
                    "trending_topics": list(reddit_data.get('trending_topics', {}).keys())[:5],
                    "data_source": "real_api"
                }
                
                self.logger.info(f"âœ… Reddit ì‹¤ì œ ë°ì´í„°: {reddit_summary.get('total_content', 0)}ê°œ ì½˜í…ì¸ ")
                
            except Exception as e:
                self.logger.error(f"âŒ Reddit ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                social_data["platforms"]["reddit"] = {
                    "posts": 0,
                    "comments": 0,
                    "error": str(e),
                    "data_source": "failed"
                }
        else:
            # Reddit ë¹„í™œì„±í™” ì‹œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            social_data["platforms"]["reddit"] = {
                "posts": 45,
                "comments": 892,
                "sentiment": {"positive": 35, "negative": 25, "neutral": 40},
                "top_subreddits": ["r/investing", "r/stocks", "r/economics"],
                "data_source": "simulation"
            }
        
        # Twitter ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° (ì‹¤ì œ API ì—°ë™ ì‹œ êµì²´)
        social_data["platforms"]["twitter"] = {
            "mentions": 1250,
            "sentiment": {"positive": 45, "negative": 30, "neutral": 25},
            "trending_hashtags": ["#Fed", "#Inflation", "#StockMarket", "#Economy", "#Bitcoin"],
            "data_source": "simulation"
        }
        
        # ê¸°íƒ€ í”Œëž«í¼ ì‹œë®¬ë ˆì´ì…˜
        social_data["platforms"]["news_comments"] = {
            "total_comments": 2340,
            "sentiment": {"positive": 35, "negative": 40, "neutral": 25},
            "data_source": "simulation"
        }
        
        # ì „ì²´ ê°ì • ê³„ì‚°
        social_data["overall_sentiment"] = self._calculate_overall_social_sentiment(social_data["platforms"])
        
        return social_data
    
    def _calculate_overall_social_sentiment(self, platforms: Dict[str, Any]) -> Dict[str, Any]:
        """ì†Œì…œë¯¸ë””ì–´ ì „ì²´ ê°ì • ê³„ì‚°"""
        try:
            total_positive = 0
            total_negative = 0
            total_neutral = 0
            total_weight = 0
            
            # Reddit ì‹¤ì œ ë°ì´í„° ê°€ì¤‘ì¹˜ ë†’ê²Œ
            if "reddit" in platforms and platforms["reddit"].get("data_source") == "real_api":
                reddit_sentiment = platforms["reddit"].get("sentiment", {})
                reddit_weight = 0.6  # ì‹¤ì œ ë°ì´í„°ì´ë¯€ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜
                
                total_positive += reddit_sentiment.get("positive", 0) * reddit_weight
                total_negative += reddit_sentiment.get("negative", 0) * reddit_weight
                total_neutral += reddit_sentiment.get("neutral", 0) * reddit_weight
                total_weight += reddit_weight
            
            # Twitter ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            if "twitter" in platforms:
                twitter_sentiment = platforms["twitter"].get("sentiment", {})
                twitter_weight = 0.3
                
                total_positive += twitter_sentiment.get("positive", 0) * twitter_weight
                total_negative += twitter_sentiment.get("negative", 0) * twitter_weight
                total_neutral += twitter_sentiment.get("neutral", 0) * twitter_weight
                total_weight += twitter_weight
            
            # ê¸°íƒ€ í”Œëž«í¼
            if "news_comments" in platforms:
                news_sentiment = platforms["news_comments"].get("sentiment", {})
                news_weight = 0.1
                
                total_positive += news_sentiment.get("positive", 0) * news_weight
                total_negative += news_sentiment.get("negative", 0) * news_weight
                total_neutral += news_sentiment.get("neutral", 0) * news_weight
                total_weight += news_weight
            
            if total_weight > 0:
                avg_positive = total_positive / total_weight
                avg_negative = total_negative / total_weight
                avg_neutral = total_neutral / total_weight
                
                # ê°ì • ì ìˆ˜ ê³„ì‚° (-1 to 1)
                sentiment_score = (avg_positive - avg_negative) / 100
                
                # ë¼ë²¨ë§
                if sentiment_score > 0.1:
                    label = "positive"
                elif sentiment_score < -0.1:
                    label = "negative"
                else:
                    label = "neutral"
                
                return {
                    "score": round(sentiment_score, 3),
                    "label": label,
                    "confidence": 0.75,
                    "distribution": {
                        "positive": round(avg_positive, 1),
                        "negative": round(avg_negative, 1),
                        "neutral": round(avg_neutral, 1)
                    }
                }
            
        except Exception as e:
            self.logger.error(f"ì „ì²´ ì†Œì…œë¯¸ë””ì–´ ê°ì • ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        # ê¸°ë³¸ê°’
        return {
            "score": 0.0,
            "label": "neutral",
            "confidence": 0.5,
            "distribution": {"positive": 33.3, "negative": 33.3, "neutral": 33.3}
        }

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ðŸ“° ê°•í™”ëœ ë‰´ìŠ¤ ë° ì†Œì…œë¯¸ë””ì–´ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    collector = EnhancedNewsCollector()
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘
    news_data = collector.collect_news_by_category(max_items_per_source=5)
    
    # ê²°ê³¼ ì¶œë ¥
    summary = news_data.get("summary", {})
    print(f"\nðŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"  ì´ ê¸°ì‚¬: {summary.get('total_articles', 0)}ê°œ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
    by_category = summary.get("by_category", {})
    for category, stats in by_category.items():
        print(f"  {category}: {stats['count']}ê°œ (ê¸ì •: {stats['positive']}, ë¶€ì •: {stats['negative']})")
    
    # ê°ì • ë¶„ì„
    sentiment = summary.get("sentiment_analysis", {})
    if sentiment:
        print(f"\nðŸ’­ ì „ì²´ ê°ì • ë¶„ì„:")
        print(f"  ê¸ì •: {sentiment.get('positive_ratio', 0)}%")
        print(f"  ë¶€ì •: {sentiment.get('negative_ratio', 0)}%")
    
    # íŠ¸ë Œë”© ì£¼ì œ
    trending = summary.get("trending_topics", {})
    if trending:
        print(f"\nðŸ”¥ íŠ¸ë Œë”© ì£¼ì œ:")
        for topic, count in list(trending.items())[:5]:
            print(f"  {topic}: {count}íšŒ ì–¸ê¸‰")
    
    # ì†Œì…œë¯¸ë””ì–´ ë°ì´í„°
    social_data = collector.get_social_media_mentions()
    print(f"\nðŸ“± ì†Œì…œë¯¸ë””ì–´ ë¶„ì„:")
    print(f"  ì „ì²´ ê°ì •: {social_data['overall_sentiment']['label']}")
    print(f"  Twitter ì–¸ê¸‰: {social_data['platforms']['twitter']['mentions']}íšŒ")

if __name__ == "__main__":
    main()
