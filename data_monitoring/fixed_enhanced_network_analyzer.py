#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ ê°œì„ ëœ ê²½ì œ ê°œë… ê¸°ë°˜ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ëª¨ë“ˆ
ì‹¤ì œ Reddit ë°ì´í„°ì™€ ì—°ë™í•˜ì—¬ ì—ëŸ¬ ì—†ì´ ì‘ë™
"""

import networkx as nx
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter, defaultdict
import re
import json
import sys
import os

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class FixedEnhancedNetworkAnalyzer:
    """ìˆ˜ì •ëœ ê°œì„ ëœ ê²½ì œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # 16ê°œ ì£¼ìš” ê²½ì œ ì¹´í…Œê³ ë¦¬ì™€ ì„¸ë¶€ ê°œë…ë“¤ (ê°„ì†Œí™”)
        self.economic_concepts = {
            # 1. í†µí™”ì •ì±…
            'monetary_policy': {
                'main_concepts': ['í†µí™”ì •ì±…', 'ê¸ˆë¦¬ì •ì±…', 'ê¸°ì¤€ê¸ˆë¦¬', 'Fed', 'Federal Reserve'],
                'related_terms': ['ê¸ˆë¦¬ì¸ìƒ', 'ê¸ˆë¦¬ì¸í•˜', 'ì–‘ì ì™„í™”', 'QE', 'ê¸´ì¶•ì •ì±…', 'ì™„í™”ì •ì±…', 'FOMC'],
                'weight': 1.0
            },
            
            # 2. ì¸í”Œë ˆì´ì…˜
            'inflation': {
                'main_concepts': ['ì¸í”Œë ˆì´ì…˜', 'inflation', 'ë¬¼ê°€ìƒìŠ¹', 'CPI'],
                'related_terms': ['ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'ê·¼ì›ì¸í”Œë ˆì´ì…˜', 'ë””í”Œë ˆì´ì…˜', 'ë¬¼ê°€ì•ˆì •', 'ë¬¼ê°€ì••ë ¥'],
                'weight': 1.0
            },
            
            # 3. ì£¼ì‹ì‹œì¥
            'stock_market': {
                'main_concepts': ['ì£¼ì‹ì‹œì¥', 'stock market', 'ì¦ì‹œ', 'stocks'],
                'related_terms': ['S&P 500', 'NASDAQ', 'Dow Jones', 'ìƒìŠ¹ì¥', 'í•˜ë½ì¥', 'ë³€ë™ì„±', 'VIX'],
                'weight': 1.0
            },
            
            # 4. ê¸°ì—…ì‹¤ì 
            'corporate_performance': {
                'main_concepts': ['ê¸°ì—…ì‹¤ì ', 'ì‹¤ì ë°œí‘œ', 'earnings', 'revenue'],
                'related_terms': ['ë§¤ì¶œ', 'ìˆœì´ìµ', 'ì˜ì—…ì´ìµ', 'EPS', 'ê°€ì´ë˜ìŠ¤', 'ë¶„ê¸°ì‹¤ì '],
                'weight': 0.9
            },
            
            # 5. ê¸°ìˆ ì£¼
            'technology': {
                'main_concepts': ['ê¸°ìˆ ì£¼', 'tech stocks', 'í…Œí¬ì£¼', 'technology'],
                'related_terms': ['Apple', 'Microsoft', 'Google', 'Amazon', 'Tesla', 'ë°˜ë„ì²´', 'AI'],
                'weight': 0.9
            },
            
            # 6. ê¸ˆìœµì„¹í„°
            'financial_sector': {
                'main_concepts': ['ê¸ˆìœµì„¹í„°', 'financial sector', 'ì€í–‰ì£¼', 'banking'],
                'related_terms': ['JPMorgan', 'Goldman Sachs', 'ìˆœì´ìë§ˆì§„', 'ëŒ€ì¶œ', 'ì‹ ìš©ìœ„í—˜'],
                'weight': 0.8
            },
            
            # 7. ì—ë„ˆì§€
            'energy': {
                'main_concepts': ['ì—ë„ˆì§€', 'energy', 'ì›ìœ ', 'oil'],
                'related_terms': ['WTI', 'Brent', 'OPEC', 'ì²œì—°ê°€ìŠ¤', 'ì‹ ì¬ìƒì—ë„ˆì§€', 'íƒœì–‘ê´‘'],
                'weight': 0.8
            },
            
            # 8. ë¶€ë™ì‚°
            'real_estate': {
                'main_concepts': ['ë¶€ë™ì‚°', 'real estate', 'ì£¼íƒì‹œì¥', 'housing'],
                'related_terms': ['ì£¼íƒê°€ê²©', 'ëª¨ê¸°ì§€', 'REIT', 'ìƒì—…ìš©ë¶€ë™ì‚°', 'ì£¼íƒë‹´ë³´ëŒ€ì¶œ'],
                'weight': 0.8
            },
            
            # 9. êµ­ì œë¬´ì—­
            'international_trade': {
                'main_concepts': ['êµ­ì œë¬´ì—­', 'trade', 'ë¬´ì—­ì „ìŸ', 'tariff'],
                'related_terms': ['ê´€ì„¸', 'ìˆ˜ì¶œ', 'ìˆ˜ì…', 'ë¬´ì—­ìˆ˜ì§€', 'ê³µê¸‰ë§', 'ê¸€ë¡œë²Œí™”'],
                'weight': 0.8
            },
            
            # 10. ì•”í˜¸í™”í
            'cryptocurrency': {
                'main_concepts': ['ì•”í˜¸í™”í', 'cryptocurrency', 'ë¹„íŠ¸ì½”ì¸', 'bitcoin'],
                'related_terms': ['Ethereum', 'ë¸”ë¡ì²´ì¸', 'DeFi', 'NFT', 'ë””ì§€í„¸ìì‚°', 'crypto'],
                'weight': 0.7
            },
            
            # 11. ESG
            'esg': {
                'main_concepts': ['ESG', 'ì§€ì†ê°€ëŠ¥ì„±', 'sustainability', 'ì¹œí™˜ê²½'],
                'related_terms': ['íƒ„ì†Œì¤‘ë¦½', 'ê¸°í›„ë³€í™”', 'ê·¸ë¦°ì—ë„ˆì§€', 'ì‚¬íšŒì ì±…ì„', 'ì§€ë°°êµ¬ì¡°'],
                'weight': 0.7
            },
            
            # 12. ê³ ìš©ì‹œì¥
            'labor_market': {
                'main_concepts': ['ê³ ìš©ì‹œì¥', 'labor market', 'ì‹¤ì—…ë¥ ', 'unemployment'],
                'related_terms': ['ë¹„ë†ì—…ê³ ìš©', 'êµ¬ì¸', 'ì„ê¸ˆìƒìŠ¹', 'ë…¸ë™ì°¸ì—¬ìœ¨', 'ì¼ìë¦¬ì°½ì¶œ'],
                'weight': 0.8
            },
            
            # 13. ì†Œë¹„
            'consumer_spending': {
                'main_concepts': ['ì†Œë¹„', 'consumer spending', 'ì†Œë§¤íŒë§¤', 'retail'],
                'related_terms': ['ì†Œë¹„ìì‹ ë¢°', 'ê°œì¸ì†Œë¹„', 'ì†Œë¹„ì‹¬ë¦¬', 'ê°€ê³„ì†Œë“', 'ì €ì¶•ë¥ '],
                'weight': 0.8
            },
            
            # 14. ì •ë¶€ì •ì±…
            'government_policy': {
                'main_concepts': ['ì •ë¶€ì •ì±…', 'government policy', 'ì¬ì •ì •ì±…', 'fiscal'],
                'related_terms': ['ì •ë¶€ì§€ì¶œ', 'ì„¸ê¸ˆ', 'ë¶€ì±„', 'ì ì', 'ì˜ˆì‚°', 'ë¶€ì–‘ì±…', 'ê·œì œ'],
                'weight': 0.8
            },
            
            # 15. ì§€ì •í•™ì  ë¦¬ìŠ¤í¬
            'geopolitical_risk': {
                'main_concepts': ['ì§€ì •í•™ì ë¦¬ìŠ¤í¬', 'geopolitical', 'êµ­ì œì •ì¹˜', 'war'],
                'related_terms': ['ì „ìŸ', 'ë¶„ìŸ', 'ì œì¬', 'ì™¸êµ', 'ì•ˆë³´', 'ì •ì¹˜ë¶ˆì•ˆ', 'ì„ ê±°'],
                'weight': 0.7
            },
            
            # 16. ì‹œì¥ì‹¬ë¦¬
            'market_sentiment': {
                'main_concepts': ['ì‹œì¥ì‹¬ë¦¬', 'market sentiment', 'íˆ¬ìì‹¬ë¦¬', 'sentiment'],
                'related_terms': ['ê³µí¬', 'íƒìš•', 'ë‚™ê´€', 'ë¹„ê´€', 'ìœ„í—˜íšŒí”¼', 'íˆ¬ììì‹ ë¢°'],
                'weight': 0.7
            }
        }
        
        # ê´€ê³„ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        self.relationship_weights = {
            'strong_correlation': 1.0,
            'moderate_correlation': 0.7,
            'weak_correlation': 0.4,
            'causal_relationship': 0.9,
            'inverse_relationship': 0.8,
            'temporal_relationship': 0.6,
            'mentioned_together': 0.3
        }
        
        self.logger.info("âœ… ìˆ˜ì •ëœ ê°œì„ ëœ ê²½ì œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_economic_concepts(self, text: str) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê²½ì œ ê°œë… ì¶”ì¶œ (ì—ëŸ¬ ìˆ˜ì • ë²„ì „)"""
        
        if not text or not isinstance(text, str):
            return {}, {}
        
        text_lower = text.lower()
        # HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text_clean = re.sub(r'<[^>]+>', '', text)
        text_clean = re.sub(r'[^\w\sê°€-í£]', ' ', text_clean)
        text_clean_lower = text_clean.lower()
        
        found_concepts = {}
        concept_scores = {}
        
        for category, concept_data in self.economic_concepts.items():
            category_score = 0
            found_terms = []
            
            try:
                # ì£¼ìš” ê°œë… ê²€ìƒ‰ (ë†’ì€ ê°€ì¤‘ì¹˜)
                for main_concept in concept_data['main_concepts']:
                    if main_concept.lower() in text_clean_lower:
                        category_score += 2.0 * concept_data['weight']
                        found_terms.append(main_concept)
                
                # ê´€ë ¨ ìš©ì–´ ê²€ìƒ‰ (ë‚®ì€ ê°€ì¤‘ì¹˜)
                for related_term in concept_data['related_terms']:
                    if related_term.lower() in text_clean_lower:
                        category_score += 1.0 * concept_data['weight']
                        found_terms.append(related_term)
                
                if category_score > 0:
                    found_concepts[category] = {
                        'score': category_score,
                        'terms': list(set(found_terms)),
                        'weight': concept_data['weight']
                    }
                    concept_scores[category] = category_score
                    
            except Exception as e:
                self.logger.warning(f"ê°œë… ì¶”ì¶œ ì˜¤ë¥˜ ({category}): {e}")
                continue
        
        return found_concepts, concept_scores
    
    def analyze_concept_relationships(self, texts: List[str]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì—ì„œ ê²½ì œ ê°œë… ê°„ ê´€ê³„ ë¶„ì„ (ì—ëŸ¬ ìˆ˜ì • ë²„ì „)"""
        
        if not texts or not isinstance(texts, list):
            return {'error': 'Invalid input texts'}
        
        self.logger.info("ğŸ” ê²½ì œ ê°œë… ê´€ê³„ ë¶„ì„ ì‹œì‘")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ ê°œë… ì¶”ì¶œ
        all_concepts = {}
        concept_cooccurrence = defaultdict(lambda: defaultdict(int))
        concept_sentiments = defaultdict(list)
        
        valid_texts = [text for text in texts if text and isinstance(text, str) and len(text.strip()) > 10]
        
        if not valid_texts:
            return {'error': 'No valid texts found'}
        
        for text in valid_texts:
            try:
                concepts, scores = self.extract_economic_concepts(text)
                
                # ê°„ë‹¨í•œ ê°ì • ë¶„ì„
                sentiment = self._simple_sentiment_analysis(text)
                
                # ê°œë…ë³„ ì ìˆ˜ ëˆ„ì 
                for concept, data in concepts.items():
                    if concept not in all_concepts:
                        all_concepts[concept] = {
                            'total_score': 0,
                            'mention_count': 0,
                            'terms': set(),
                            'weight': data['weight']
                        }
                    
                    all_concepts[concept]['total_score'] += data['score']
                    all_concepts[concept]['mention_count'] += 1
                    all_concepts[concept]['terms'].update(data['terms'])
                    concept_sentiments[concept].append(sentiment)
                
                # ë™ì‹œ ì¶œí˜„ ê´€ê³„ ê³„ì‚°
                concept_list = list(concepts.keys())
                for i, concept1 in enumerate(concept_list):
                    for concept2 in concept_list[i+1:]:
                        # ìƒí˜¸ ê°€ì¤‘ì¹˜ ì ìš©
                        weight = (concepts[concept1]['score'] * concepts[concept2]['score']) ** 0.5
                        concept_cooccurrence[concept1][concept2] += weight
                        concept_cooccurrence[concept2][concept1] += weight
                        
            except Exception as e:
                self.logger.warning(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        if not all_concepts:
            return {'error': 'No economic concepts found'}
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        # ë…¸ë“œ ì¶”ê°€ (ê°œë…ë“¤)
        for concept, data in all_concepts.items():
            try:
                avg_sentiment = np.mean(concept_sentiments[concept]) if concept_sentiments[concept] else 0
                
                G.add_node(concept, 
                          score=data['total_score'],
                          mentions=data['mention_count'],
                          terms=list(data['terms']),
                          weight=data['weight'],
                          sentiment=float(avg_sentiment),
                          size=min(data['total_score'] * 10, 100))
                          
            except Exception as e:
                self.logger.warning(f"ë…¸ë“œ ì¶”ê°€ ì˜¤ë¥˜ ({concept}): {e}")
                continue
        
        # ì—£ì§€ ì¶”ê°€ (ê´€ê³„ë“¤)
        edges_added = 0
        for concept1, connections in concept_cooccurrence.items():
            for concept2, weight in connections.items():
                if weight > 0.5 and concept1 in G.nodes() and concept2 in G.nodes():
                    try:
                        # ê´€ê³„ ìœ í˜• ê²°ì •
                        relationship_type = self._determine_relationship_type_safe(concept1, concept2, weight)
                        
                        # ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜
                        normalized_weight = min(weight / 10.0, 1.0)
                        
                        G.add_edge(concept1, concept2,
                                  weight=float(normalized_weight),
                                  relationship_type=relationship_type,
                                  strength=float(weight))
                        edges_added += 1
                        
                    except Exception as e:
                        self.logger.warning(f"ì—£ì§€ ì¶”ê°€ ì˜¤ë¥˜ ({concept1}-{concept2}): {e}")
                        continue
        
        # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._calculate_network_metrics_safe(G)
        
        self.logger.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ: {len(G.nodes())}ê°œ ë…¸ë“œ, {len(G.edges())}ê°œ ì—£ì§€")
        
        return {
            'graph': G,
            'concepts': all_concepts,
            'metrics': metrics,
            'node_count': len(G.nodes()),
            'edge_count': len(G.edges()),
            'concept_sentiments': dict(concept_sentiments)
        }
    
    def _simple_sentiment_analysis(self, text: str) -> float:
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (TextBlob ì˜ì¡´ì„± ì œê±°)"""
        
        try:
            # ê¸ì •ì  í‚¤ì›Œë“œ
            positive_words = [
                'good', 'great', 'excellent', 'positive', 'growth', 'profit', 'success',
                'opportunity', 'optimistic', 'recovery', 'improvement', 'bullish'
            ]
            
            # ë¶€ì •ì  í‚¤ì›Œë“œ
            negative_words = [
                'bad', 'terrible', 'negative', 'loss', 'crash', 'decline', 'recession',
                'crisis', 'worry', 'concern', 'risk', 'problem', 'bearish'
            ]
            
            text_lower = text.lower()
            positive_count = sum(1 for word in positive_words if word in text_lower)
            negative_count = sum(1 for word in negative_words if word in text_lower)
            
            if positive_count > negative_count:
                return 0.3
            elif negative_count > positive_count:
                return -0.3
            else:
                return 0.0
                
        except Exception as e:
            self.logger.warning(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return 0.0
    
    def _determine_relationship_type_safe(self, concept1: str, concept2: str, weight: float) -> str:
        """ì•ˆì „í•œ ê´€ê³„ ìœ í˜• ê²°ì •"""
        
        try:
            # íŠ¹ì • ê°œë… ìŒì— ëŒ€í•œ ê´€ê³„ ìœ í˜• ì •ì˜
            strong_correlations = {
                ('monetary_policy', 'inflation'): 'causal_relationship',
                ('inflation', 'stock_market'): 'inverse_relationship',
                ('technology', 'stock_market'): 'strong_correlation',
                ('geopolitical_risk', 'market_sentiment'): 'causal_relationship',
                ('labor_market', 'consumer_spending'): 'strong_correlation',
                ('government_policy', 'market_sentiment'): 'moderate_correlation',
                ('energy', 'inflation'): 'strong_correlation',
                ('real_estate', 'monetary_policy'): 'strong_correlation',
                ('cryptocurrency', 'market_sentiment'): 'strong_correlation',
                ('esg', 'technology'): 'moderate_correlation'
            }
            
            # ìˆœì„œ ë¬´ê´€í•˜ê²Œ ê²€ìƒ‰
            pair1 = (concept1, concept2)
            pair2 = (concept2, concept1)
            
            if pair1 in strong_correlations:
                return strong_correlations[pair1]
            elif pair2 in strong_correlations:
                return strong_correlations[pair2]
            
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê´€ê³„ ìœ í˜• ê²°ì •
            if weight > 5.0:
                return 'strong_correlation'
            elif weight > 3.0:
                return 'moderate_correlation'
            elif weight > 1.0:
                return 'weak_correlation'
            else:
                return 'mentioned_together'
                
        except Exception as e:
            self.logger.warning(f"ê´€ê³„ ìœ í˜• ê²°ì • ì˜¤ë¥˜: {e}")
            return 'mentioned_together'
    
    def _calculate_network_metrics_safe(self, G: nx.Graph) -> Dict[str, Any]:
        """ì•ˆì „í•œ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        if len(G.nodes()) == 0:
            return {'error': 'Empty graph'}
        
        metrics = {}
        
        try:
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            metrics['density'] = float(nx.density(G))
            metrics['average_clustering'] = float(nx.average_clustering(G))
            
            # ì¤‘ì‹¬ì„± ì§€í‘œ (ì•ˆì „í•˜ê²Œ ê³„ì‚°)
            if len(G.nodes()) > 1:
                degree_centrality = nx.degree_centrality(G)
                betweenness_centrality = nx.betweenness_centrality(G)
                closeness_centrality = nx.closeness_centrality(G)
                
                metrics['centrality'] = {
                    'degree': {k: float(v) for k, v in degree_centrality.items()},
                    'betweenness': {k: float(v) for k, v in betweenness_centrality.items()},
                    'closeness': {k: float(v) for k, v in closeness_centrality.items()}
                }
                
                # ê°€ì¥ ì¤‘ìš”í•œ ë…¸ë“œë“¤
                metrics['top_nodes'] = {
                    'by_degree': sorted(degree_centrality.items(), 
                                      key=lambda x: x[1], reverse=True)[:5],
                    'by_betweenness': sorted(betweenness_centrality.items(), 
                                           key=lambda x: x[1], reverse=True)[:5]
                }
            
            # ì—°ê²° ì„±ë¶„
            if nx.is_connected(G):
                metrics['diameter'] = nx.diameter(G)
                metrics['average_path_length'] = float(nx.average_shortest_path_length(G))
            else:
                components = list(nx.connected_components(G))
                metrics['connected_components'] = len(components)
                metrics['largest_component_size'] = len(max(components, key=len))
            
        except Exception as e:
            self.logger.warning(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            metrics['error'] = str(e)
        
        return metrics
    
    def generate_network_insights(self, network_result: Dict[str, Any]) -> List[str]:
        """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼ì—ì„œ ì¸ì‚¬ì´íŠ¸ ìƒì„± (ì•ˆì „í•œ ë²„ì „)"""
        
        insights = []
        
        try:
            if 'error' in network_result:
                return [f"ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {network_result['error']}"]
            
            G = network_result.get('graph')
            metrics = network_result.get('metrics', {})
            concepts = network_result.get('concepts', {})
            
            if not G or len(G.nodes()) == 0:
                return ["ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²½ì œ ê°œë…ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."]
            
            # 1. ì „ì²´ ë„¤íŠ¸ì›Œí¬ ê·œëª¨
            insights.append(f"ğŸ“Š ì´ {len(G.nodes())}ê°œì˜ ê²½ì œ ê°œë…ê³¼ {len(G.edges())}ê°œì˜ ê´€ê³„ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            
            # 2. ê°€ì¥ ì¤‘ìš”í•œ ê°œë…ë“¤
            if 'centrality' in metrics and 'degree' in metrics['centrality']:
                top_concepts = sorted(metrics['centrality']['degree'].items(), 
                                    key=lambda x: x[1], reverse=True)[:3]
                concept_names = [self._get_concept_display_name(concept) for concept, _ in top_concepts]
                insights.append(f"ğŸ¯ ê°€ì¥ ì¤‘ìš”í•œ ê²½ì œ ê°œë…: {', '.join(concept_names)}")
            
            # 3. ë„¤íŠ¸ì›Œí¬ ë°€ë„
            if 'density' in metrics:
                density = metrics['density']
                if density > 0.3:
                    insights.append("ğŸ”— ê²½ì œ ê°œë…ë“¤ ê°„ì˜ ì—°ê²°ì´ ë§¤ìš° ë°€ì ‘í•©ë‹ˆë‹¤.")
                elif density > 0.1:
                    insights.append("ğŸ”— ê²½ì œ ê°œë…ë“¤ ê°„ì˜ ì—°ê²°ì´ ì ë‹¹í•©ë‹ˆë‹¤.")
                else:
                    insights.append("ğŸ”— ê²½ì œ ê°œë…ë“¤ ê°„ì˜ ì—°ê²°ì´ ëŠìŠ¨í•©ë‹ˆë‹¤.")
            
            # 4. ê°ì • ë¶„ì„ ê²°ê³¼
            concept_sentiments = network_result.get('concept_sentiments', {})
            if concept_sentiments:
                positive_concepts = []
                negative_concepts = []
                
                for concept, sentiments in concept_sentiments.items():
                    if sentiments:
                        avg_sentiment = np.mean(sentiments)
                        if avg_sentiment > 0.1:
                            positive_concepts.append(self._get_concept_display_name(concept))
                        elif avg_sentiment < -0.1:
                            negative_concepts.append(self._get_concept_display_name(concept))
                
                if positive_concepts:
                    insights.append(f"ğŸ˜Š ê¸ì •ì  ì–¸ê¸‰: {', '.join(positive_concepts[:3])}")
                if negative_concepts:
                    insights.append(f"ğŸ˜Ÿ ë¶€ì •ì  ì–¸ê¸‰: {', '.join(negative_concepts[:3])}")
            
            # 5. ì£¼ìš” ê´€ê³„ë“¤
            if G.edges():
                strong_relationships = []
                for edge in G.edges(data=True):
                    if edge[2].get('weight', 0) > 0.7:
                        source_name = self._get_concept_display_name(edge[0])
                        target_name = self._get_concept_display_name(edge[1])
                        strong_relationships.append(f"{source_name} â†” {target_name}")
                
                if strong_relationships:
                    insights.append(f"ğŸ”¥ ê°•í•œ ì—°ê´€ì„±: {', '.join(strong_relationships[:2])}")
            
        except Exception as e:
            self.logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            insights.append("ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        return insights if insights else ["ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
    
    def _get_concept_display_name(self, concept_key: str) -> str:
        """ê°œë… í‚¤ë¥¼ í‘œì‹œìš© ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        
        display_names = {
            'monetary_policy': 'í†µí™”ì •ì±…',
            'inflation': 'ì¸í”Œë ˆì´ì…˜',
            'stock_market': 'ì£¼ì‹ì‹œì¥',
            'corporate_performance': 'ê¸°ì—…ì‹¤ì ',
            'technology': 'ê¸°ìˆ ì£¼',
            'financial_sector': 'ê¸ˆìœµì„¹í„°',
            'energy': 'ì—ë„ˆì§€',
            'real_estate': 'ë¶€ë™ì‚°',
            'international_trade': 'êµ­ì œë¬´ì—­',
            'cryptocurrency': 'ì•”í˜¸í™”í',
            'esg': 'ESG',
            'labor_market': 'ê³ ìš©ì‹œì¥',
            'consumer_spending': 'ì†Œë¹„',
            'government_policy': 'ì •ë¶€ì •ì±…',
            'geopolitical_risk': 'ì§€ì •í•™ì  ë¦¬ìŠ¤í¬',
            'market_sentiment': 'ì‹œì¥ì‹¬ë¦¬'
        }
        
        return display_names.get(concept_key, concept_key.replace('_', ' ').title())

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    analyzer = FixedEnhancedNetworkAnalyzer()
    
    # ì‹¤ì œ Reddit ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    try:
        from real_reddit_collector import RealRedditCollector
        
        print("ğŸ§ª ìˆ˜ì •ëœ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        # Reddit ë°ì´í„° ìˆ˜ì§‘
        reddit_collector = RealRedditCollector()
        texts = reddit_collector.get_texts_for_network_analysis(max_posts=20)
        
        print(f"ğŸ“± Redditì—ì„œ ìˆ˜ì§‘í•œ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")
        
        if texts:
            # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤í–‰
            result = analyzer.analyze_concept_relationships(texts)
            
            if 'error' not in result:
                print(f"âœ… ë¶„ì„ ì„±ê³µ:")
                print(f"   ë…¸ë“œ ìˆ˜: {result['node_count']}")
                print(f"   ì—£ì§€ ìˆ˜: {result['edge_count']}")
                
                # ì¸ì‚¬ì´íŠ¸ ìƒì„±
                insights = analyzer.generate_network_insights(result)
                print(f"\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:")
                for insight in insights:
                    print(f"   â€¢ {insight}")
            else:
                print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result['error']}")
        else:
            print("âŒ Reddit ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ë°±ì—…: ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        print("\nğŸ”„ ìƒ˜í”Œ ë°ì´í„°ë¡œ ë°±ì—… í…ŒìŠ¤íŠ¸:")
        sample_texts = [
            "ì—°ì¤€ì´ ê¸°ì¤€ê¸ˆë¦¬ë¥¼ ì¸ìƒí•˜ë©° ì¸í”Œë ˆì´ì…˜ ì–µì œì— ë‚˜ì„°ë‹¤",
            "ê¸°ìˆ ì£¼ê°€ í•˜ë½ì„¸ë¥¼ ë³´ì´ë©° ì£¼ì‹ì‹œì¥ì´ ë¶ˆì•ˆì •í•˜ë‹¤",
            "ë¹„íŠ¸ì½”ì¸ ê°€ê²©ì´ ìƒìŠ¹í•˜ë©° ì•”í˜¸í™”í ì‹œì¥ì´ íšŒë³µì„¸ë‹¤"
        ]
        
        result = analyzer.analyze_concept_relationships(sample_texts)
        if 'error' not in result:
            print(f"âœ… ë°±ì—… í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result['node_count']}ê°œ ë…¸ë“œ")
        else:
            print(f"âŒ ë°±ì—… í…ŒìŠ¤íŠ¸ë„ ì‹¤íŒ¨: {result['error']}")
