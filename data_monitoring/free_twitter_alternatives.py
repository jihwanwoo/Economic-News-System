#!/usr/bin/env python3
"""
ë¬´ë£Œ Twitter ëŒ€ì•ˆ ë°ì´í„° ìˆ˜ì§‘ê¸°
API í‚¤ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ë²•ë“¤
"""

import requests
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import time
import json
import re
from bs4 import BeautifulSoup
import feedparser

class FreeTwitterAlternatives:
    """ë¬´ë£Œ Twitter ëŒ€ì•ˆ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # User-Agent ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        self.logger.info("âœ… ë¬´ë£Œ Twitter ëŒ€ì•ˆ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def collect_alternative_social_data(self) -> Dict[str, Any]:
        """ë‹¤ì–‘í•œ ë¬´ë£Œ ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘"""
        
        self.logger.info("ğŸ” ë¬´ë£Œ ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        all_data = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'sources': {},
            'total_posts': 0
        }
        
        # 1. Reddit ë°ì´í„° (ë¬´ë£Œ)
        reddit_data = self._collect_reddit_data()
        if reddit_data:
            all_data['sources']['reddit'] = reddit_data
            all_data['total_posts'] += len(reddit_data.get('posts', []))
        
        # 2. Nitter (Twitter í”„ë¡ì‹œ) - ë¬´ë£Œ
        nitter_data = self._collect_nitter_data()
        if nitter_data:
            all_data['sources']['nitter'] = nitter_data
            all_data['total_posts'] += len(nitter_data.get('posts', []))
        
        # 3. ê²½ì œ ë‰´ìŠ¤ ëŒ“ê¸€ ìˆ˜ì§‘
        news_comments = self._collect_news_comments()
        if news_comments:
            all_data['sources']['news_comments'] = news_comments
            all_data['total_posts'] += len(news_comments.get('comments', []))
        
        # 4. ê²½ì œ í¬ëŸ¼ ë°ì´í„°
        forum_data = self._collect_forum_data()
        if forum_data:
            all_data['sources']['forums'] = forum_data
            all_data['total_posts'] += len(forum_data.get('posts', []))
        
        # 5. ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° (ë°±ì—…)
        if all_data['total_posts'] == 0:
            simulation_data = self._generate_realistic_simulation()
            all_data['sources']['simulation'] = simulation_data
            all_data['total_posts'] = len(simulation_data.get('posts', []))
            all_data['status'] = 'simulation'
        
        # ìš”ì•½ ìƒì„±
        all_data['summary'] = self._generate_combined_summary(all_data['sources'])
        
        self.logger.info(f"âœ… ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {all_data['total_posts']}ê°œ í¬ìŠ¤íŠ¸")
        return all_data
    
    def _collect_reddit_data(self) -> Optional[Dict[str, Any]]:
        """Reddit ë°ì´í„° ìˆ˜ì§‘ (ë¬´ë£Œ API)"""
        try:
            self.logger.info("ğŸ“± Reddit ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # Redditì˜ ê²½ì œ ê´€ë ¨ ì„œë¸Œë ˆë”§ë“¤
            subreddits = [
                'economics', 'investing', 'stocks', 'cryptocurrency',
                'personalfinance', 'SecurityAnalysis', 'ValueInvesting'
            ]
            
            all_posts = []
            
            for subreddit in subreddits:
                try:
                    # Reddit JSON API ì‚¬ìš© (ë¬´ë£Œ, ì œí•œì )
                    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=10"
                    response = requests.get(url, headers=self.headers, timeout=10)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        for post in data['data']['children']:
                            post_data = post['data']
                            
                            all_posts.append({
                                'id': post_data['id'],
                                'title': post_data['title'],
                                'text': post_data.get('selftext', ''),
                                'score': post_data['score'],
                                'num_comments': post_data['num_comments'],
                                'created_utc': datetime.fromtimestamp(post_data['created_utc']),
                                'subreddit': subreddit,
                                'url': f"https://reddit.com{post_data['permalink']}",
                                'author': post_data.get('author', '[deleted]')
                            })
                    
                    time.sleep(1)  # Rate limiting
                    
                except Exception as e:
                    self.logger.warning(f"Reddit {subreddit} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    continue
            
            return {
                'source': 'reddit',
                'posts': all_posts,
                'total_posts': len(all_posts),
                'subreddits': subreddits
            }
            
        except Exception as e:
            self.logger.error(f"Reddit ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _collect_nitter_data(self) -> Optional[Dict[str, Any]]:
        """Nitter (Twitter í”„ë¡ì‹œ) ë°ì´í„° ìˆ˜ì§‘"""
        try:
            self.logger.info("ğŸ¦ Nitter ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # Nitter ì¸ìŠ¤í„´ìŠ¤ë“¤ (ë¬´ë£Œ Twitter í”„ë¡ì‹œ)
            nitter_instances = [
                'nitter.net',
                'nitter.it',
                'nitter.unixfox.eu'
            ]
            
            # ê²½ì œ ê´€ë ¨ Twitter ê³„ì •ë“¤
            economic_accounts = [
                'federalreserve', 'ecb', 'bankofengland',
                'federalreserve', 'treasurydept', 'sec_news'
            ]
            
            all_tweets = []
            
            for instance in nitter_instances:
                try:
                    for account in economic_accounts[:2]:  # ì œí•œì ìœ¼ë¡œ ìˆ˜ì§‘
                        url = f"https://{instance}/{account}"
                        response = requests.get(url, headers=self.headers, timeout=10)
                        
                        if response.status_code == 200:
                            # HTML íŒŒì‹±ìœ¼ë¡œ íŠ¸ìœ— ì¶”ì¶œ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
                            soup = BeautifulSoup(response.content, 'html.parser')
                            tweets = soup.find_all('div', class_='tweet-content')
                            
                            for i, tweet in enumerate(tweets[:5]):  # ìµœëŒ€ 5ê°œ
                                all_tweets.append({
                                    'id': f"{account}_{i}",
                                    'text': tweet.get_text().strip(),
                                    'account': account,
                                    'source': 'nitter',
                                    'created_at': datetime.now() - timedelta(hours=i),
                                    'instance': instance
                                })
                        
                        time.sleep(2)  # Rate limiting
                        break  # ì²« ë²ˆì§¸ ì„±ê³µí•œ ì¸ìŠ¤í„´ìŠ¤ë§Œ ì‚¬ìš©
                    
                    if all_tweets:
                        break  # ë°ì´í„°ë¥¼ ì–»ì—ˆìœ¼ë©´ ì¤‘ë‹¨
                        
                except Exception as e:
                    self.logger.warning(f"Nitter {instance} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    continue
            
            if all_tweets:
                return {
                    'source': 'nitter',
                    'posts': all_tweets,
                    'total_posts': len(all_tweets),
                    'accounts': economic_accounts
                }
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"Nitter ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _collect_news_comments(self) -> Optional[Dict[str, Any]]:
        """ê²½ì œ ë‰´ìŠ¤ ëŒ“ê¸€ ìˆ˜ì§‘"""
        try:
            self.logger.info("ğŸ’¬ ë‰´ìŠ¤ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")
            
            # ê²½ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë“¤ (ëŒ“ê¸€ì´ ìˆëŠ” ê³³)
            news_sites = [
                {
                    'name': 'Yahoo Finance',
                    'rss': 'https://feeds.finance.yahoo.com/rss/2.0/headline',
                    'comment_selector': '.comment-text'
                }
            ]
            
            all_comments = []
            
            for site in news_sites:
                try:
                    # RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    feed = feedparser.parse(site['rss'])
                    
                    for entry in feed.entries[:3]:  # ìµœëŒ€ 3ê°œ ê¸°ì‚¬
                        # ì‹¤ì œ ëŒ“ê¸€ ìˆ˜ì§‘ì€ ë³µì¡í•˜ë¯€ë¡œ ì‹œë®¬ë ˆì´ì…˜
                        simulated_comments = self._generate_news_comments(entry.title)
                        all_comments.extend(simulated_comments)
                    
                except Exception as e:
                    self.logger.warning(f"{site['name']} ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    continue
            
            return {
                'source': 'news_comments',
                'comments': all_comments,
                'total_comments': len(all_comments)
            }
            
        except Exception as e:
            self.logger.error(f"ë‰´ìŠ¤ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _collect_forum_data(self) -> Optional[Dict[str, Any]]:
        """ê²½ì œ í¬ëŸ¼ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            self.logger.info("ğŸ›ï¸ ê²½ì œ í¬ëŸ¼ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # ì‹œë®¬ë ˆì´ì…˜ í¬ëŸ¼ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ì›¹ ìŠ¤í¬ë˜í•‘ í•„ìš”)
            forum_posts = [
                {
                    'id': f'forum_{i}',
                    'title': title,
                    'content': content,
                    'author': f'user_{i}',
                    'created_at': datetime.now() - timedelta(hours=i),
                    'replies': max(0, 20 - i * 2),
                    'views': max(0, 500 - i * 50)
                }
                for i, (title, content) in enumerate([
                    ("Fed ê¸ˆë¦¬ ì •ì±… ì „ë§", "ë‹¤ìŒ FOMC íšŒì˜ì—ì„œ ê¸ˆë¦¬ ë™ê²°ì´ ì˜ˆìƒë©ë‹ˆë‹¤..."),
                    ("ì¸í”Œë ˆì´ì…˜ ì§€í‘œ ë¶„ì„", "ìµœê·¼ CPI ë°ì´í„°ë¥¼ ë³´ë©´ ë¬¼ê°€ ìƒìŠ¹ì„¸ê°€ ë‘”í™”ë˜ê³  ìˆì–´..."),
                    ("ê¸°ìˆ ì£¼ íˆ¬ì ì „ëµ", "í˜„ì¬ ê¸°ìˆ ì£¼ ë°¸ë¥˜ì—ì´ì…˜ì´ ë§¤ë ¥ì ì¸ ìˆ˜ì¤€ê¹Œì§€ ë‚´ë ¤ì™”ëŠ”ë°..."),
                    ("ë¶€ë™ì‚° ì‹œì¥ ì „ë§", "ê¸ˆë¦¬ ì¸ìƒ ì‚¬ì´í´ì´ ëë‚˜ê°€ë©´ì„œ ë¶€ë™ì‚° ì‹œì¥ë„ ì•ˆì •í™”ë  ê²ƒìœ¼ë¡œ..."),
                    ("ì•”í˜¸í™”í ê·œì œ ë™í–¥", "SECì˜ ì•”í˜¸í™”í ê·œì œ ë°©í–¥ì´ ì ì°¨ ëª…í™•í•´ì§€ê³  ìˆì–´...")
                ])
            ]
            
            return {
                'source': 'forums',
                'posts': forum_posts,
                'total_posts': len(forum_posts)
            }
            
        except Exception as e:
            self.logger.error(f"í¬ëŸ¼ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_news_comments(self, news_title: str) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•œ ì‹œë®¬ë ˆì´ì…˜ ëŒ“ê¸€ ìƒì„±"""
        
        # ë‰´ìŠ¤ ì œëª© ê¸°ë°˜ ê´€ë ¨ ëŒ“ê¸€ í…œí”Œë¦¿
        comment_templates = [
            "ì´ëŸ° ìƒí™©ì—ì„œëŠ” íˆ¬ìë¥¼ ì¡°ì‹¬í•´ì•¼ê² ë„¤ìš”",
            "ì˜ˆìƒí–ˆë˜ ê²°ê³¼ì…ë‹ˆë‹¤. ì‹œì¥ì´ ì–´ë–»ê²Œ ë°˜ì‘í• ì§€ ê¶ê¸ˆí•˜ë„¤ìš”",
            "ì •ë¶€ ì •ì±…ì´ ë” í•„ìš”í•œ ì‹œì ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤",
            "ì¥ê¸°ì ìœ¼ë¡œëŠ” ì¢‹ì€ ì‹ í˜¸ì¼ ìˆ˜ ìˆê² ì–´ìš”",
            "ì „ë¬¸ê°€ë“¤ì˜ ì˜ê²¬ì´ ì—‡ê°ˆë¦¬ê³  ìˆëŠ” ìƒí™©ì´ë„¤ìš”"
        ]
        
        comments = []
        for i, template in enumerate(comment_templates):
            comments.append({
                'id': f'comment_{i}',
                'text': template,
                'news_title': news_title,
                'author': f'reader_{i}',
                'created_at': datetime.now() - timedelta(minutes=i * 30),
                'likes': max(0, 10 - i * 2)
            })
        
        return comments
    
    def _generate_realistic_simulation(self) -> Dict[str, Any]:
        """í˜„ì‹¤ì ì¸ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±"""
        
        simulation_posts = [
            {
                'id': f'sim_{i}',
                'text': text,
                'platform': platform,
                'author': f'user_{i}',
                'created_at': datetime.now() - timedelta(hours=i),
                'engagement': {
                    'likes': max(0, 100 - i * 10),
                    'shares': max(0, 20 - i * 2),
                    'comments': max(0, 15 - i)
                },
                'sentiment': {
                    'polarity': (i % 3 - 1) * 0.4,
                    'label': ['negative', 'neutral', 'positive'][i % 3]
                }
            }
            for i, (text, platform) in enumerate([
                ("ì—°ì¤€ ê¸ˆë¦¬ ì¸ìƒìœ¼ë¡œ ì£¼ì‹ì‹œì¥ ë³€ë™ì„± í™•ëŒ€ ì˜ˆìƒ", "twitter"),
                ("ë¹„íŠ¸ì½”ì¸ ê°€ê²© íšŒë³µì„¸, ê¸°ê´€ íˆ¬ìì ìœ ì… ì¦ê°€", "reddit"),
                ("ë¶€ë™ì‚° ì‹œì¥ ì•ˆì •í™” ì‹ í˜¸, ê±°ë˜ëŸ‰ ì¦ê°€ ê´€ì°°", "forum"),
                ("ê³ ìš©ì§€í‘œ ê°œì„ ìœ¼ë¡œ ì†Œë¹„ íšŒë³µ ê¸°ëŒ€ê° ìƒìŠ¹", "news_comment"),
                ("ì¸í”Œë ˆì´ì…˜ ë‘”í™” ì¡°ì§, ì—°ì°©ë¥™ ì‹œë‚˜ë¦¬ì˜¤ ë¶€ê°", "twitter"),
                ("ê¸°ìˆ ì£¼ ë°¸ë¥˜ì—ì´ì…˜ ë§¤ë ¥ë„ ì¦ê°€, ì €ì  ë§¤ìˆ˜ ê¸°íšŒ", "reddit"),
                ("ì—ë„ˆì§€ ê°€ê²© ì•ˆì •í™”ë¡œ ë¬¼ê°€ ì••ë ¥ ì™„í™” ì „ë§", "forum"),
                ("ë‹¬ëŸ¬ ê°•ì„¸ ì§€ì†, ì‹ í¥êµ­ í†µí™” ì•½ì„¸ ìš°ë ¤", "news_comment"),
                ("ESG íˆ¬ì íŠ¸ë Œë“œ ì§€ì†, ì¹œí™˜ê²½ ê¸°ì—… ì£¼ëª©", "twitter"),
                ("ì¤‘ì•™ì€í–‰ ì •ì±… ì „í™˜ì  ì„ë°•, ì‹œì¥ ê´€ì‹¬ ì§‘ì¤‘", "reddit")
            ])
        ]
        
        return {
            'source': 'simulation',
            'posts': simulation_posts,
            'total_posts': len(simulation_posts),
            'note': 'API í‚¤ê°€ ì—†ì–´ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤'
        }
    
    def _generate_combined_summary(self, sources: Dict[str, Any]) -> Dict[str, Any]:
        """í†µí•© ìš”ì•½ ìƒì„±"""
        
        total_posts = 0
        all_sentiments = []
        platform_distribution = {}
        
        for source_name, source_data in sources.items():
            posts = source_data.get('posts', []) or source_data.get('comments', [])
            total_posts += len(posts)
            
            platform_distribution[source_name] = len(posts)
            
            # ê°ì • ë¶„ì„ ì§‘ê³„
            for post in posts:
                if 'sentiment' in post:
                    all_sentiments.append(post['sentiment']['polarity'])
        
        # ì „ì²´ ê°ì • ë¶„ì„
        avg_sentiment = sum(all_sentiments) / len(all_sentiments) if all_sentiments else 0
        
        return {
            'total_posts': total_posts,
            'platform_distribution': platform_distribution,
            'sentiment_analysis': {
                'average_sentiment': avg_sentiment,
                'sentiment_label': 'positive' if avg_sentiment > 0.1 else 'negative' if avg_sentiment < -0.1 else 'neutral',
                'total_analyzed': len(all_sentiments)
            },
            'data_sources': list(sources.keys())
        }

# ì‚¬ìš© ê°€ì´ë“œ
def usage_guide():
    """ì‚¬ìš© ê°€ì´ë“œ ì¶œë ¥"""
    guide = """
    ğŸ†“ ë¬´ë£Œ Twitter ëŒ€ì•ˆ ë°ì´í„° ìˆ˜ì§‘ ê°€ì´ë“œ
    
    âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ë¬´ë£Œ ì†ŒìŠ¤:
    
    1. ğŸ“± Reddit API (ë¬´ë£Œ)
       - ê²½ì œ ê´€ë ¨ ì„œë¸Œë ˆë”§ ë°ì´í„°
       - ì œí•œ: 60 ìš”ì²­/ë¶„
       - ë“±ë¡ ë¶ˆí•„ìš”
    
    2. ğŸ¦ Nitter (Twitter í”„ë¡ì‹œ)
       - Twitter ë°ì´í„°ë¥¼ ë¬´ë£Œë¡œ ì ‘ê·¼
       - ì œí•œ: ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
       - HTML íŒŒì‹± í•„ìš”
    
    3. ğŸ’¬ ë‰´ìŠ¤ ëŒ“ê¸€
       - ê²½ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ëŒ“ê¸€
       - RSS í”¼ë“œ + ì›¹ ìŠ¤í¬ë˜í•‘
       - ì‚¬ì´íŠ¸ë³„ ì œí•œ ì¡´ì¬
    
    4. ğŸ›ï¸ ê²½ì œ í¬ëŸ¼
       - íˆ¬ì/ê²½ì œ ê´€ë ¨ í¬ëŸ¼
       - ì›¹ ìŠ¤í¬ë˜í•‘ í•„ìš”
       - ì‚¬ì´íŠ¸ ì •ì±… í™•ì¸ í•„ìš”
    
    âš ï¸ ì£¼ì˜ì‚¬í•­:
    - ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œ robots.txt í™•ì¸
    - Rate limiting ì¤€ìˆ˜
    - ì‚¬ì´íŠ¸ ì´ìš©ì•½ê´€ í™•ì¸
    - ê°œì¸ì •ë³´ ë³´í˜¸ ê³ ë ¤
    
    ğŸ’¡ ê¶Œì¥ì‚¬í•­:
    - ì—¬ëŸ¬ ì†ŒìŠ¤ ì¡°í•© ì‚¬ìš©
    - ìºì‹±ìœ¼ë¡œ API í˜¸ì¶œ ìµœì†Œí™”
    - ì—ëŸ¬ ì²˜ë¦¬ ë° ë°±ì—… ë°ì´í„° ì¤€ë¹„
    """
    print(guide)

if __name__ == "__main__":
    # ì‚¬ìš© ê°€ì´ë“œ ì¶œë ¥
    usage_guide()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    collector = FreeTwitterAlternatives()
    result = collector.collect_alternative_social_data()
    
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
    print(f"ìƒíƒœ: {result['status']}")
    print(f"ì´ í¬ìŠ¤íŠ¸: {result['total_posts']}")
    print(f"ë°ì´í„° ì†ŒìŠ¤: {', '.join(result['summary']['data_sources'])}")
    
    if 'note' in result.get('sources', {}).get('simulation', {}):
        print(f"ì°¸ê³ : {result['sources']['simulation']['note']}")
