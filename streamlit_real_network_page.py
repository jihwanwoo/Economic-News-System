#!/usr/bin/env python3
"""
ì‹¤ì œ Reddit ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê°œì„ ëœ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ Streamlit í˜ì´ì§€
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import networkx as nx
import numpy as np
from datetime import datetime
import sys
import os
import json

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_monitoring.fixed_enhanced_network_analyzer import FixedEnhancedNetworkAnalyzer
from data_monitoring.real_reddit_collector import RealRedditCollector

def create_real_network_page():
    """ì‹¤ì œ Reddit ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í˜ì´ì§€"""
    
    st.title("ğŸ•¸ï¸ ì‹¤ì œ Reddit ë°ì´í„° ê¸°ë°˜ ê²½ì œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
        
        # ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •
        st.subheader("ğŸ“± Reddit ë°ì´í„° ìˆ˜ì§‘")
        max_posts = st.slider("ìµœëŒ€ í¬ìŠ¤íŠ¸ ìˆ˜", 10, 100, 30, 10)
        
        # ë„¤íŠ¸ì›Œí¬ ì„¤ì •
        st.subheader("ğŸ•¸ï¸ ë„¤íŠ¸ì›Œí¬ ì„¤ì •")
        min_edge_weight = st.slider("ìµœì†Œ ì—°ê²° ê°•ë„", 0.1, 1.0, 0.3, 0.1)
        max_nodes = st.slider("ìµœëŒ€ ë…¸ë“œ ìˆ˜", 5, 30, 15, 5)
        layout_type = st.selectbox("ë ˆì´ì•„ì›ƒ", ["spring", "circular", "kamada_kawai"])
        
        # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
        if st.button("ğŸ” ì‹¤ì œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰", type="primary", key="real_network_analysis"):
            st.session_state.run_real_analysis = True
        
        # Reddit ì—°ê²° ìƒíƒœ í‘œì‹œ
        st.markdown("---")
        st.subheader("ğŸ“Š Reddit ì—°ê²° ìƒíƒœ")
        
        try:
            collector = RealRedditCollector()
            st.success("âœ… Reddit API ì—°ê²° ì„±ê³µ")
            st.info("ğŸ“± ì‹¤ì œ ê²½ì œ ì„œë¸Œë ˆë”§ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
        except Exception as e:
            st.error(f"âŒ Reddit ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            st.warning("âš ï¸ .env íŒŒì¼ì˜ Reddit API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“Š ì‹¤ì œ Reddit ë°ì´í„° ê¸°ë°˜ ê²½ì œ ê°œë… ë„¤íŠ¸ì›Œí¬")
        
        # ë¶„ì„ ì‹¤í–‰
        if st.session_state.get('run_real_analysis', False):
            with st.spinner("ì‹¤ì œ Reddit ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘..."):
                network_data = run_real_network_analysis(max_posts, min_edge_weight, max_nodes)
                st.session_state.real_network_data = network_data
                st.session_state.run_real_analysis = False
        
        # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
        if 'real_network_data' in st.session_state:
            network_data = st.session_state.real_network_data
            
            if 'error' not in network_data:
                # ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ
                st.info(f"ğŸ“± ì‹¤ì œ Reddit ë°ì´í„° {network_data.get('text_count', 0)}ê°œ í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼")
                
                # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
                fig = create_real_network_visualization(network_data, layout_type, min_edge_weight)
                st.plotly_chart(fig, use_container_width=True, height=600)
                
                # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ í‘œì‹œ
                display_real_network_metrics(network_data)
                
                # Reddit ë°ì´í„° ìƒì„¸ ì •ë³´
                if 'reddit_stats' in network_data:
                    display_reddit_stats(network_data['reddit_stats'])
                    
            else:
                st.error(f"ë¶„ì„ ì˜¤ë¥˜: {network_data['error']}")
                st.info("ğŸ’¡ .env íŒŒì¼ì˜ Reddit API í‚¤ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”")
        else:
            # ì´ˆê¸° ìƒíƒœ
            st.info("ğŸ‘† ì‚¬ì´ë“œë°”ì—ì„œ 'ì‹¤ì œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
            
            # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
            display_system_info()
    
    with col2:
        st.subheader("ğŸ“ˆ ë¶„ì„ ê²°ê³¼")
        
        if 'real_network_data' in st.session_state and 'error' not in st.session_state.real_network_data:
            network_data = st.session_state.real_network_data
            
            # ì£¼ìš” ì¸ì‚¬ì´íŠ¸
            st.markdown("### ğŸ¯ ì‹¤ì œ ë°ì´í„° ì¸ì‚¬ì´íŠ¸")
            analyzer = FixedEnhancedNetworkAnalyzer()
            insights = analyzer.generate_network_insights(network_data)
            
            for insight in insights:
                st.markdown(f"â€¢ {insight}")
            
            # ìƒìœ„ ê°œë…ë“¤
            st.markdown("### ğŸ† í•µì‹¬ ê²½ì œ ê°œë…")
            display_top_concepts(network_data)
            
            # ê´€ê³„ ìœ í˜• ë¶„í¬
            st.markdown("### ğŸ”— ê´€ê³„ ìœ í˜• ë¶„í¬")
            display_relationship_distribution(network_data)
            
            # Reddit ì„œë¸Œë ˆë”§ë³„ ê¸°ì—¬ë„
            if 'reddit_stats' in network_data:
                st.markdown("### ğŸ“± ì„œë¸Œë ˆë”§ë³„ ê¸°ì—¬ë„")
                display_subreddit_contribution(network_data['reddit_stats'])
        
        else:
            st.markdown("### ğŸ“‹ ë¶„ì„ ëŒ€ê¸° ì¤‘")
            st.info("ì‹¤ì œ Reddit ë°ì´í„° ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ ì—¬ê¸°ì— ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
            
            # ê°œì„  ì‚¬í•­ ì„¤ëª…
            st.markdown("### âœ¨ ì‹¤ì œ ë°ì´í„° ë¶„ì„ì˜ ì¥ì ")
            st.markdown("""
            **ğŸš€ ì‹¤ì œ ë°ì´í„° ì‚¬ìš©:**
            - âœ… **ì‹¤ì œ Reddit í¬ìŠ¤íŠ¸**: ê°€ìƒ ë°ì´í„° ì—†ìŒ
            - âœ… **ì‹¤ì‹œê°„ ê²½ì œ í† ë¡ **: í˜„ì¬ ì´ìŠˆ ë°˜ì˜
            - âœ… **ë‹¤ì–‘í•œ ê´€ì **: 8ê°œ ê²½ì œ ì„œë¸Œë ˆë”§
            - âœ… **ê°ì • ë¶„ì„**: ì‹¤ì œ íˆ¬ìì ì‹¬ë¦¬ ë°˜ì˜
            - âœ… **ì‹ ë¢°ì„±**: API í‚¤ ê¸°ë°˜ ê³µì‹ ë°ì´í„°
            """)

@st.cache_data(ttl=300)
def run_real_network_analysis(max_posts: int, min_edge_weight: float, max_nodes: int) -> dict:
    """ì‹¤ì œ Reddit ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤í–‰"""
    
    try:
        # Reddit ë°ì´í„° ìˆ˜ì§‘
        collector = RealRedditCollector()
        texts = collector.get_texts_for_network_analysis(max_posts=max_posts)
        
        if not texts:
            return {'error': 'Reddit ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ - í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ì¶”ê°€ í†µê³„ ì •ë³´ ìˆ˜ì§‘
        posts_data = collector.collect_economic_posts(max_posts_per_subreddit=max_posts//8)
        reddit_stats = posts_data.get('subreddit_stats', {})
        
        # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤í–‰
        analyzer = FixedEnhancedNetworkAnalyzer()
        network_result = analyzer.analyze_concept_relationships(texts)
        
        if 'error' in network_result:
            return network_result
        
        # ë…¸ë“œ ìˆ˜ ì œí•œ
        if network_result.get('graph') and len(network_result['graph'].nodes()) > max_nodes:
            G = network_result['graph']
            node_scores = [(node, data.get('score', 0)) for node, data in G.nodes(data=True)]
            top_nodes = sorted(node_scores, key=lambda x: x[1], reverse=True)[:max_nodes]
            top_node_names = [node for node, _ in top_nodes]
            
            # ì„œë¸Œê·¸ë˜í”„ ìƒì„±
            subG = G.subgraph(top_node_names).copy()
            network_result['graph'] = subG
            network_result['node_count'] = len(subG.nodes())
            network_result['edge_count'] = len(subG.edges())
        
        # ì¶”ê°€ ì •ë³´ í¬í•¨
        network_result['text_count'] = len(texts)
        network_result['reddit_stats'] = reddit_stats
        network_result['data_source'] = 'Real Reddit Data'
        
        return network_result
        
    except Exception as e:
        return {'error': f'ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'}

def create_real_network_visualization(network_data: dict, layout_type: str, min_edge_weight: float):
    """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ìƒì„±"""
    
    G = network_data['graph']
    
    if len(G.nodes()) == 0:
        fig = go.Figure()
        fig.add_annotation(text="ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤", 
                          xref="paper", yref="paper",
                          x=0.5, y=0.5, showarrow=False)
        return fig
    
    # ë ˆì´ì•„ì›ƒ ê³„ì‚°
    if layout_type == "spring":
        pos = nx.spring_layout(G, k=2, iterations=50)
    elif layout_type == "circular":
        pos = nx.circular_layout(G)
    else:  # kamada_kawai
        pos = nx.kamada_kawai_layout(G)
    
    # ì—£ì§€ íŠ¸ë ˆì´ìŠ¤ ìƒì„±
    edge_x = []
    edge_y = []
    edge_weights = []
    
    for edge in G.edges(data=True):
        if edge[2].get('weight', 0) >= min_edge_weight:
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            edge_weights.append(edge[2].get('weight', 0))
    
    edge_trace = go.Scatter(x=edge_x, y=edge_y,
                           line=dict(width=1, color='#888'),
                           hoverinfo='none',
                           mode='lines')
    
    # ë…¸ë“œ íŠ¸ë ˆì´ìŠ¤ ìƒì„±
    node_x = []
    node_y = []
    node_text = []
    node_info = []
    node_size = []
    node_color = []
    
    # ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ ë§¤í•‘
    category_colors = {
        'monetary_policy': '#FF6B6B',
        'inflation': '#4ECDC4',
        'stock_market': '#45B7D1',
        'corporate_performance': '#96CEB4',
        'technology': '#FFEAA7',
        'financial_sector': '#DDA0DD',
        'energy': '#FFA07A',
        'real_estate': '#98D8C8',
        'international_trade': '#F7DC6F',
        'cryptocurrency': '#BB8FCE',
        'esg': '#85C1E9',
        'labor_market': '#F8C471',
        'consumer_spending': '#82E0AA',
        'government_policy': '#F1948A',
        'geopolitical_risk': '#D7DBDD',
        'market_sentiment': '#AED6F1'
    }
    
    analyzer = FixedEnhancedNetworkAnalyzer()
    
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        
        # ë…¸ë“œ ì •ë³´
        node_data = G.nodes[node]
        score = node_data.get('score', 0)
        mentions = node_data.get('mentions', 0)
        sentiment = node_data.get('sentiment', 0)
        terms = node_data.get('terms', [])
        
        # í‘œì‹œ ì´ë¦„
        display_name = analyzer._get_concept_display_name(node)
        node_text.append(display_name)
        
        # í˜¸ë²„ ì •ë³´
        sentiment_emoji = "ğŸ˜Š" if sentiment > 0.1 else "ğŸ˜Ÿ" if sentiment < -0.1 else "ğŸ˜"
        info = f"<b>{display_name}</b><br>"
        info += f"ì ìˆ˜: {score:.1f}<br>"
        info += f"Reddit ì–¸ê¸‰: {mentions}íšŒ<br>"
        info += f"ê°ì •: {sentiment_emoji} ({sentiment:.2f})<br>"
        info += f"ê´€ë ¨ ìš©ì–´: {', '.join(terms[:3])}"
        node_info.append(info)
        
        # ë…¸ë“œ í¬ê¸° (ì ìˆ˜ ê¸°ë°˜)
        size = max(15, min(score * 8, 60))
        node_size.append(size)
        
        # ë…¸ë“œ ìƒ‰ìƒ (ì¹´í…Œê³ ë¦¬ ê¸°ë°˜)
        color = category_colors.get(node, '#888888')
        node_color.append(color)
    
    node_trace = go.Scatter(x=node_x, y=node_y,
                           mode='markers+text',
                           hoverinfo='text',
                           text=node_text,
                           hovertext=node_info,
                           textposition="middle center",
                           marker=dict(size=node_size,
                                     color=node_color,
                                     line=dict(width=2, color='white')))
    
    # ê·¸ë˜í”„ ìƒì„±
    fig = go.Figure(data=[edge_trace, node_trace],
                   layout=go.Layout(
                        title=dict(
                            text=f'ì‹¤ì œ Reddit ë°ì´í„° ê¸°ë°˜ ê²½ì œ ê°œë… ë„¤íŠ¸ì›Œí¬<br>({len(G.nodes())}ê°œ ë…¸ë“œ, {len(G.edges())}ê°œ ì—°ê²°)',
                            font=dict(size=16)
                        ),
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20,l=5,r=5,t=60),
                        annotations=[ dict(
                            text="ğŸ“± ì‹¤ì œ Reddit ë°ì´í„° | ë…¸ë“œ í¬ê¸°: ì–¸ê¸‰ ë¹ˆë„ | ìƒ‰ìƒ: ê²½ì œ ì¹´í…Œê³ ë¦¬",
                            showarrow=False,
                            xref="paper", yref="paper",
                            x=0.005, y=-0.002,
                            xanchor='left', yanchor='bottom',
                            font=dict(size=10)
                        )],
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        height=600))
    
    return fig

def display_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ"""
    st.info("ğŸ¯ **ì‹¤ì œ Reddit ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **ğŸ“± ë°ì´í„° ì†ŒìŠ¤:**
        - r/economics (ê²½ì œí•™)
        - r/investing (íˆ¬ì)
        - r/stocks (ì£¼ì‹)
        - r/personalfinance (ê°œì¸ê¸ˆìœµ)
        """)
    
    with col2:
        st.markdown("""
        **ğŸ” ë¶„ì„ ê¸°ëŠ¥:**
        - ì‹¤ì‹œê°„ Reddit í¬ìŠ¤íŠ¸ ìˆ˜ì§‘
        - ê²½ì œ ê°œë… ìë™ ì¶”ì¶œ
        - ê°ì • ë¶„ì„ í†µí•©
        - ë„¤íŠ¸ì›Œí¬ ê´€ê³„ ë¶„ì„
        """)

def display_real_network_metrics(network_data: dict):
    """ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ í‘œì‹œ"""
    metrics = network_data.get('metrics', {})
    
    if not metrics:
        return
    
    st.markdown("### ğŸ“Š ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ë…¸ë“œ ìˆ˜", network_data.get('node_count', 0))
    
    with col2:
        st.metric("ì—°ê²° ìˆ˜", network_data.get('edge_count', 0))
    
    with col3:
        density = metrics.get('density', 0)
        st.metric("ë„¤íŠ¸ì›Œí¬ ë°€ë„", f"{density:.3f}")
    
    with col4:
        text_count = network_data.get('text_count', 0)
        st.metric("ë¶„ì„ í…ìŠ¤íŠ¸", f"{text_count}ê°œ")

def display_top_concepts(network_data: dict):
    """ìƒìœ„ ê°œë…ë“¤ í‘œì‹œ"""
    metrics = network_data.get('metrics', {})
    
    if 'top_nodes' in metrics:
        top_by_degree = metrics['top_nodes'].get('by_degree', [])
        
        if top_by_degree:
            analyzer = FixedEnhancedNetworkAnalyzer()
            
            for i, (concept, centrality) in enumerate(top_by_degree[:5], 1):
                display_name = analyzer._get_concept_display_name(concept)
                st.markdown(f"{i}. **{display_name}** (ì¤‘ì‹¬ì„±: {centrality:.3f})")

def display_relationship_distribution(network_data: dict):
    """ê´€ê³„ ìœ í˜• ë¶„í¬ í‘œì‹œ"""
    G = network_data.get('graph')
    
    if not G:
        return
    
    # ê´€ê³„ ìœ í˜• ì§‘ê³„
    relationship_counts = {}
    for _, _, data in G.edges(data=True):
        rel_type = data.get('relationship_type', 'unknown')
        relationship_counts[rel_type] = relationship_counts.get(rel_type, 0) + 1
    
    if relationship_counts:
        # ë°” ì°¨íŠ¸ ìƒì„±
        fig = px.bar(
            x=list(relationship_counts.keys()),
            y=list(relationship_counts.values()),
            title="ê´€ê³„ ìœ í˜•ë³„ ë¶„í¬"
        )
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)

def display_reddit_stats(reddit_stats: dict):
    """Reddit í†µê³„ í‘œì‹œ"""
    st.markdown("### ğŸ“± Reddit ë°ì´í„° ìˆ˜ì§‘ í†µê³„")
    
    successful_subreddits = []
    failed_subreddits = []
    
    for subreddit, stats in reddit_stats.items():
        if 'error' in stats:
            failed_subreddits.append(subreddit)
        else:
            successful_subreddits.append((subreddit, stats.get('posts_collected', 0)))
    
    # ì„±ê³µí•œ ì„œë¸Œë ˆë”§
    if successful_subreddits:
        st.markdown("**âœ… ìˆ˜ì§‘ ì„±ê³µ:**")
        for subreddit, count in successful_subreddits:
            st.markdown(f"â€¢ r/{subreddit}: {count}ê°œ í¬ìŠ¤íŠ¸")
    
    # ì‹¤íŒ¨í•œ ì„œë¸Œë ˆë”§
    if failed_subreddits:
        st.markdown("**âŒ ìˆ˜ì§‘ ì‹¤íŒ¨:**")
        for subreddit in failed_subreddits:
            st.markdown(f"â€¢ r/{subreddit}")

def display_subreddit_contribution(reddit_stats: dict):
    """ì„œë¸Œë ˆë”§ë³„ ê¸°ì—¬ë„ í‘œì‹œ"""
    
    subreddit_data = []
    for subreddit, stats in reddit_stats.items():
        if 'error' not in stats:
            subreddit_data.append({
                'subreddit': f"r/{subreddit}",
                'posts': stats.get('posts_collected', 0),
                'subscribers': stats.get('subscribers', 0)
            })
    
    if subreddit_data:
        df = pd.DataFrame(subreddit_data)
        
        # í¬ìŠ¤íŠ¸ ìˆ˜ ê¸°ì¤€ íŒŒì´ ì°¨íŠ¸
        fig = px.pie(df, values='posts', names='subreddit', 
                    title="ì„œë¸Œë ˆë”§ë³„ í¬ìŠ¤íŠ¸ ê¸°ì—¬ë„")
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)

if __name__ == "__main__":
    create_real_network_page()
