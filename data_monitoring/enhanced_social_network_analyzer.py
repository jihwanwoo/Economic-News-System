#!/usr/bin/env python3
"""
ê°•í™”ëœ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸°
ê²½ì œ ê°œë… ì¤‘ì‹¬ì˜ ì˜ë¯¸ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•
"""

import networkx as nx
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter, defaultdict
import re
from textblob import TextBlob

class EnhancedSocialNetworkAnalyzer:
    """ê°•í™”ëœ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° - ê²½ì œ ê°œë… ì¤‘ì‹¬"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # í™•ì¥ëœ ê²½ì œ ê°œë… ì‚¬ì „
        self.economic_concepts = {
            # í†µí™” ì •ì±…
            'monetary_policy': [
                'federal reserve', 'fed', 'interest rates', 'interest rate', 'rates',
                'monetary policy', 'quantitative easing', 'qe', 'money supply',
                'central bank', 'jerome powell', 'fomc', 'fed funds rate'
            ],
            
            # ì¸í”Œë ˆì´ì…˜
            'inflation': [
                'inflation', 'deflation', 'cpi', 'consumer price index',
                'price increases', 'cost of living', 'purchasing power',
                'pce', 'core inflation', 'wage inflation'
            ],
            
            # ê³ ìš© ì‹œì¥
            'employment': [
                'unemployment', 'jobs', 'employment', 'labor market',
                'job market', 'hiring', 'layoffs', 'wage growth',
                'labor force', 'jobless claims', 'nonfarm payrolls'
            ],
            
            # ì£¼ì‹ ì‹œì¥
            'stock_market': [
                'stock market', 'stocks', 'equity', 'shares', 'nasdaq',
                'sp500', 's&p 500', 'dow jones', 'market volatility',
                'bull market', 'bear market', 'correction', 'rally'
            ],
            
            # ê¸°ì—… ì‹¤ì 
            'earnings': [
                'earnings', 'revenue', 'profit', 'quarterly results',
                'eps', 'earnings per share', 'guidance', 'outlook',
                'financial results', 'income statement'
            ],
            
            # ê²½ì œ ì„±ì¥
            'economic_growth': [
                'gdp', 'economic growth', 'recession', 'expansion',
                'economic recovery', 'growth rate', 'productivity',
                'economic indicators', 'business cycle'
            ],
            
            # ê¸ˆìœµ ì‹œì¥
            'financial_markets': [
                'bond market', 'treasury', 'yield curve', 'credit markets',
                'corporate bonds', 'municipal bonds', 'fixed income',
                'bond yields', 'treasury rates'
            ],
            
            # ì„¹í„°ë³„
            'technology': [
                'tech stocks', 'technology', 'artificial intelligence', 'ai',
                'semiconductors', 'chips', 'software', 'cloud computing',
                'cybersecurity', 'fintech'
            ],
            
            'energy': [
                'oil prices', 'crude oil', 'energy sector', 'renewable energy',
                'natural gas', 'petroleum', 'opec', 'energy stocks',
                'clean energy', 'solar', 'wind power'
            ],
            
            'healthcare': [
                'healthcare', 'pharmaceuticals', 'biotech', 'medical devices',
                'drug development', 'clinical trials', 'fda approval',
                'health insurance', 'medicare', 'medicaid'
            ],
            
            'financial_services': [
                'banks', 'banking', 'financial services', 'credit',
                'lending', 'mortgages', 'insurance', 'fintech',
                'payment systems', 'digital payments'
            ],
            
            # êµ­ì œ ê²½ì œ
            'international_trade': [
                'trade war', 'tariffs', 'exports', 'imports',
                'trade deficit', 'trade surplus', 'wto', 'nafta',
                'supply chain', 'globalization'
            ],
            
            'currency': [
                'dollar', 'euro', 'yen', 'yuan', 'currency',
                'exchange rate', 'forex', 'dollar strength',
                'currency devaluation', 'currency appreciation'
            ],
            
            # íˆ¬ì ê´€ë ¨
            'investment': [
                'investment', 'portfolio', 'asset allocation', 'diversification',
                'risk management', 'hedge funds', 'mutual funds', 'etf',
                'index funds', 'active investing', 'passive investing'
            ],
            
            'cryptocurrency': [
                'bitcoin', 'ethereum', 'cryptocurrency', 'crypto',
                'blockchain', 'digital currency', 'defi', 'nft',
                'crypto market', 'altcoins'
            ],
            
            # ì •ì±… ê´€ë ¨
            'fiscal_policy': [
                'fiscal policy', 'government spending', 'budget deficit',
                'national debt', 'stimulus', 'infrastructure spending',
                'tax policy', 'tax cuts', 'tax increases'
            ],
            
            'regulation': [
                'regulation', 'regulatory', 'sec', 'compliance',
                'antitrust', 'monopoly', 'market regulation',
                'financial regulation', 'banking regulation'
            ]
        }
        
        # ê°œë… ê°„ ê´€ê³„ ê°€ì¤‘ì¹˜
        self.concept_relationships = {
            ('monetary_policy', 'inflation'): 0.9,
            ('monetary_policy', 'stock_market'): 0.8,
            ('inflation', 'employment'): 0.7,
            ('earnings', 'stock_market'): 0.9,
            ('economic_growth', 'employment'): 0.8,
            ('technology', 'stock_market'): 0.7,
            ('energy', 'inflation'): 0.6,
            ('international_trade', 'currency'): 0.8,
            ('fiscal_policy', 'economic_growth'): 0.7,
            ('regulation', 'financial_services'): 0.8
        }
        
        self.logger.info("âœ… ê°•í™”ëœ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_concepts_from_text(self, text: str) -> Dict[str, float]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê²½ì œ ê°œë… ì¶”ì¶œ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        text_lower = text.lower()
        # HTML íƒœê·¸ ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±°
        import re
        text_clean = re.sub(r'<[^>]+>', '', text)
        text_clean = re.sub(r'[^\w\s]', ' ', text_clean)
        text_clean_lower = text_clean.lower()
        
        concept_scores = {}
        
        for concept, keywords in self.economic_concepts.items():
            score = 0
            matches = []
            
            for keyword in keywords:
                # ì •í™•í•œ ë§¤ì¹­
                if keyword in text_clean_lower:
                    # í‚¤ì›Œë“œ ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ (ê¸´ í‚¤ì›Œë“œì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
                    weight = len(keyword.split()) * 1.5
                    score += weight
                    matches.append(keyword)
                
                # ë¶€ë¶„ ë§¤ì¹­ (ë‹¨ì–´ ë‹¨ìœ„)
                keyword_words = keyword.split()
                if len(keyword_words) > 1:
                    if all(word in text_clean_lower for word in keyword_words):
                        score += len(keyword_words) * 1.2
                        matches.append(keyword)
            
            if score > 0:
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ëŒ€ë¹„ ì •ê·œí™”
                normalized_score = score / (len(text_clean.split()) + 1) * 100
                concept_scores[concept] = {
                    'score': normalized_score,
                    'matches': matches,
                    'raw_score': score
                }
        
        return concept_scores
    
    def build_concept_network_from_reddit(self, reddit_data: Dict[str, Any]) -> Dict[str, Any]:
        """Reddit ë°ì´í„°ì—ì„œ ê²½ì œ ê°œë… ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•"""
        self.logger.info("ğŸ•¸ï¸ Reddit ê²½ì œ ê°œë… ë„¤íŠ¸ì›Œí¬ êµ¬ì¶• ì‹œì‘")
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        # ê°œë…ë³„ ì–¸ê¸‰ íšŸìˆ˜ ë° ê°ì •
        concept_mentions = defaultdict(int)
        concept_sentiments = defaultdict(list)
        concept_contexts = defaultdict(list)
        
        # ê°œë… ê°„ ë™ì‹œ ì¶œí˜„ ë§¤íŠ¸ë¦­ìŠ¤
        concept_cooccurrence = defaultdict(int)
        
        # Reddit ë°ì´í„° ì²˜ë¦¬
        subreddits = reddit_data.get('subreddits', {})
        
        for subreddit_name, subreddit_data in subreddits.items():
            # í¬ìŠ¤íŠ¸ ë¶„ì„
            posts = subreddit_data.get('posts', [])
            for post in posts:
                title = post.get('title', '')
                selftext = post.get('selftext', '')
                full_text = f"{title} {selftext}"
                
                if len(full_text.strip()) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                    continue
                
                # ê°œë… ì¶”ì¶œ
                concepts = self.extract_concepts_from_text(full_text)
                
                # ê°œë…ë³„ í†µê³„ ì—…ë°ì´íŠ¸
                for concept, data in concepts.items():
                    concept_mentions[concept] += 1
                    
                    # ê°ì • ì •ë³´
                    sentiment = post.get('sentiment', {})
                    concept_sentiments[concept].append(sentiment.get('polarity', 0))
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì €ì¥
                    concept_contexts[concept].append({
                        'text': full_text[:200] + "..." if len(full_text) > 200 else full_text,
                        'subreddit': subreddit_name,
                        'score': post.get('score', 0),
                        'url': post.get('permalink', '')
                    })
                
                # ê°œë… ê°„ ë™ì‹œ ì¶œí˜„ ê³„ì‚°
                concept_list = list(concepts.keys())
                for i, concept1 in enumerate(concept_list):
                    for concept2 in concept_list[i+1:]:
                        # ë‘ ê°œë…ì˜ ì ìˆ˜ ê³±ìœ¼ë¡œ ê´€ê³„ ê°•ë„ ê³„ì‚°
                        strength = concepts[concept1]['score'] * concepts[concept2]['score']
                        concept_cooccurrence[(concept1, concept2)] += strength
            
            # ëŒ“ê¸€ ë¶„ì„
            comments = subreddit_data.get('comments', [])
            for comment in comments:
                body = comment.get('body', '')
                
                if len(body.strip()) < 20:  # ë„ˆë¬´ ì§§ì€ ëŒ“ê¸€ ì œì™¸
                    continue
                
                # ê°œë… ì¶”ì¶œ
                concepts = self.extract_concepts_from_text(body)
                
                # ê°œë…ë³„ í†µê³„ ì—…ë°ì´íŠ¸ (ëŒ“ê¸€ì€ ê°€ì¤‘ì¹˜ 0.5)
                for concept, data in concepts.items():
                    concept_mentions[concept] += 0.5
                    
                    sentiment = comment.get('sentiment', {})
                    concept_sentiments[concept].append(sentiment.get('polarity', 0))
                    
                    concept_contexts[concept].append({
                        'text': body[:150] + "..." if len(body) > 150 else body,
                        'subreddit': subreddit_name,
                        'score': comment.get('score', 0),
                        'url': comment.get('permalink', ''),
                        'type': 'comment'
                    })
                
                # ëŒ“ê¸€ì˜ ê°œë… ê°„ ë™ì‹œ ì¶œí˜„ (ê°€ì¤‘ì¹˜ 0.5)
                concept_list = list(concepts.keys())
                for i, concept1 in enumerate(concept_list):
                    for concept2 in concept_list[i+1:]:
                        strength = concepts[concept1]['score'] * concepts[concept2]['score'] * 0.5
                        concept_cooccurrence[(concept1, concept2)] += strength
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ êµ¬ì„±
        # ë…¸ë“œ ì¶”ê°€ (ì–¸ê¸‰ íšŸìˆ˜ê°€ 2 ì´ìƒì¸ ê°œë…ë§Œ)
        for concept, mentions in concept_mentions.items():
            if mentions >= 2:  # ìµœì†Œ 2íšŒ ì´ìƒ ì–¸ê¸‰ëœ ê°œë…ë§Œ
                avg_sentiment = np.mean(concept_sentiments[concept]) if concept_sentiments[concept] else 0
                
                G.add_node(concept, 
                          mentions=mentions,
                          avg_sentiment=avg_sentiment,
                          sentiment_std=np.std(concept_sentiments[concept]) if len(concept_sentiments[concept]) > 1 else 0,
                          contexts=concept_contexts[concept][:5])  # ìƒìœ„ 5ê°œ ì»¨í…ìŠ¤íŠ¸ë§Œ ì €ì¥
        
        # ì—£ì§€ ì¶”ê°€
        for (concept1, concept2), strength in concept_cooccurrence.items():
            if concept1 in G.nodes() and concept2 in G.nodes() and strength > 1:
                # ê¸°ë³¸ ë™ì‹œ ì¶œí˜„ ê°€ì¤‘ì¹˜
                weight = strength
                
                # ì‚¬ì „ ì •ì˜ëœ ê´€ê³„ ê°€ì¤‘ì¹˜ ì¶”ê°€
                predefined_weight = self.concept_relationships.get((concept1, concept2)) or \
                                  self.concept_relationships.get((concept2, concept1))
                
                if predefined_weight:
                    weight *= (1 + predefined_weight)
                
                G.add_edge(concept1, concept2, 
                          weight=weight,
                          cooccurrence_count=strength,
                          relationship_type='conceptual')
        
        # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë©”íŠ¸ë¦­ ê³„ì‚°
        network_metrics = self._calculate_enhanced_network_metrics(G)
        
        # ì»¤ë®¤ë‹ˆí‹° íƒì§€
        communities = self._detect_concept_communities(G)
        
        # ì¤‘ìš”í•œ ê°œë… ì‹ë³„
        important_concepts = self._identify_important_concepts(G, concept_mentions)
        
        return {
            'graph': G,
            'network_metrics': network_metrics,
            'communities': communities,
            'important_concepts': important_concepts,
            'concept_mentions': dict(concept_mentions),
            'concept_sentiments': {k: np.mean(v) for k, v in concept_sentiments.items()},
            'total_concepts': len(G.nodes()),
            'total_relationships': len(G.edges()),
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def _calculate_enhanced_network_metrics(self, G: nx.Graph) -> Dict[str, Any]:
        """ê°•í™”ëœ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if len(G.nodes()) == 0:
            return {}
        
        metrics = {
            'nodes_count': len(G.nodes()),
            'edges_count': len(G.edges()),
            'density': nx.density(G),
            'average_clustering': nx.average_clustering(G),
        }
        
        if len(G.nodes()) > 0:
            # ì¤‘ì‹¬ì„± ì¸¡ì •
            degree_centrality = nx.degree_centrality(G)
            betweenness_centrality = nx.betweenness_centrality(G)
            closeness_centrality = nx.closeness_centrality(G)
            eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)
            
            metrics.update({
                'top_degree_centrality': sorted(degree_centrality.items(), 
                                               key=lambda x: x[1], reverse=True)[:10],
                'top_betweenness_centrality': sorted(betweenness_centrality.items(), 
                                                   key=lambda x: x[1], reverse=True)[:10],
                'top_closeness_centrality': sorted(closeness_centrality.items(), 
                                                 key=lambda x: x[1], reverse=True)[:10],
                'top_eigenvector_centrality': sorted(eigenvector_centrality.items(), 
                                                   key=lambda x: x[1], reverse=True)[:10]
            })
        
        # ì—°ê²°ì„± ë¶„ì„
        if nx.is_connected(G):
            metrics['diameter'] = nx.diameter(G)
            metrics['average_path_length'] = nx.average_shortest_path_length(G)
        else:
            metrics['connected_components'] = nx.number_connected_components(G)
            largest_cc = max(nx.connected_components(G), key=len)
            metrics['largest_component_size'] = len(largest_cc)
        
        return metrics
    
    def _detect_concept_communities(self, G: nx.Graph) -> List[List[str]]:
        """ê°œë… ì»¤ë®¤ë‹ˆí‹° íƒì§€"""
        try:
            import community as community_louvain
            partition = community_louvain.best_partition(G)
            
            communities = defaultdict(list)
            for concept, community_id in partition.items():
                communities[community_id].append(concept)
            
            # ì»¤ë®¤ë‹ˆí‹° í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_communities = sorted(communities.values(), key=len, reverse=True)
            return sorted_communities
        
        except ImportError:
            # community íŒ¨í‚¤ì§€ê°€ ì—†ìœ¼ë©´ ì—°ê²° ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
            return [list(component) for component in nx.connected_components(G)]
    
    def _identify_important_concepts(self, G: nx.Graph, concept_mentions: Dict) -> Dict[str, List[Tuple[str, float]]]:
        """ì¤‘ìš”í•œ ê°œë… ì‹ë³„"""
        if len(G.nodes()) == 0:
            return {}
        
        # ë‹¤ì–‘í•œ ì¤‘ìš”ë„ ì¸¡ì •
        degree_centrality = nx.degree_centrality(G)
        betweenness_centrality = nx.betweenness_centrality(G)
        
        # ì–¸ê¸‰ íšŸìˆ˜ ê¸°ë°˜ ì¤‘ìš”ë„
        mention_importance = {node: concept_mentions.get(node, 0) for node in G.nodes()}
        
        # ê°ì • ì˜í–¥ë ¥ (ì ˆëŒ“ê°’ì´ í´ìˆ˜ë¡ ì¤‘ìš”)
        sentiment_importance = {}
        for node in G.nodes():
            sentiment = abs(G.nodes[node].get('avg_sentiment', 0))
            sentiment_importance[node] = sentiment
        
        # ì¢…í•© ì¤‘ìš”ë„ (ì—¬ëŸ¬ ì§€í‘œì˜ ê°€ì¤‘ í‰ê· )
        combined_importance = {}
        for node in G.nodes():
            score = (
                degree_centrality.get(node, 0) * 0.3 +
                betweenness_centrality.get(node, 0) * 0.2 +
                (mention_importance.get(node, 0) / max(mention_importance.values(), default=1)) * 0.3 +
                sentiment_importance.get(node, 0) * 0.2
            )
            combined_importance[node] = score
        
        return {
            'by_degree': sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:15],
            'by_betweenness': sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:15],
            'by_mentions': sorted(mention_importance.items(), key=lambda x: x[1], reverse=True)[:15],
            'by_sentiment_impact': sorted(sentiment_importance.items(), key=lambda x: x[1], reverse=True)[:15],
            'by_combined_score': sorted(combined_importance.items(), key=lambda x: x[1], reverse=True)[:15]
        }

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ•¸ï¸ ê°•í™”ëœ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    analyzer = EnhancedSocialNetworkAnalyzer()
    
    # ìƒ˜í”Œ Reddit ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    sample_reddit_data = {
        'subreddits': {
            'investing': {
                'posts': [
                    {
                        'title': 'Fed rate hike impact on tech stocks and inflation concerns',
                        'selftext': 'The Federal Reserve decision to raise interest rates will significantly impact technology stocks. Inflation remains a key concern for investors.',
                        'sentiment': {'polarity': -0.2},
                        'score': 150,
                        'permalink': 'https://reddit.com/example1'
                    }
                ],
                'comments': [
                    {
                        'body': 'I think the stock market will see more volatility due to monetary policy changes. Earnings season will be crucial.',
                        'sentiment': {'polarity': -0.1},
                        'score': 25,
                        'permalink': 'https://reddit.com/example1/comment1'
                    }
                ]
            }
        }
    }
    
    # ê°œë… ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•
    concept_network = analyzer.build_concept_network_from_reddit(sample_reddit_data)
    
    print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"  ê°œë… ìˆ˜: {concept_network['total_concepts']}ê°œ")
    print(f"  ê´€ê³„ ìˆ˜: {concept_network['total_relationships']}ê°œ")
    
    # ì¤‘ìš”í•œ ê°œë…
    important_concepts = concept_network['important_concepts']
    combined_scores = important_concepts.get('by_combined_score', [])
    
    if combined_scores:
        print(f"\nğŸ¯ ì¤‘ìš”í•œ ê²½ì œ ê°œë… (ì¢…í•© ì ìˆ˜):")
        for i, (concept, score) in enumerate(combined_scores[:5], 1):
            print(f"   {i}. {concept.replace('_', ' ').title()}: {score:.3f}")
    
    print(f"\nâœ… ê°•í™”ëœ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
