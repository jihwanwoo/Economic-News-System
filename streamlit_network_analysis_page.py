#!/usr/bin/env python3
"""
ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ Streamlit í˜ì´ì§€
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import networkx as nx
import numpy as np
from datetime import datetime
import sys
import os

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_monitoring.social_network_analyzer import SocialNetworkAnalyzer

@st.cache_data(ttl=300)  # 5ë¶„ ìºì‹œ
def analyze_network_data(news_data):
    """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìˆ˜í–‰ (ìºì‹œë¨)"""
    try:
        analyzer = SocialNetworkAnalyzer()
        
        # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì¡° í™•ì¸ ë° ì¶”ì¶œ
        if news_data.get('status') == 'success':
            # ì‹¤ì œ ë‰´ìŠ¤ ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ì¶”ì¶œ
            news_info = news_data.get('data', {}).get('news_data', {})
            
            # ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
            news_network = analyzer.analyze_news_network(news_info)
        else:
            # ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´
            sample_news_data = {
                'categories': {
                    'financial': [
                        {
                            'title': 'Federal Reserve raises interest rates amid inflation concerns',
                            'summary': 'Jerome Powell announced the Fed decision to combat rising inflation',
                            'sentiment': {'polarity': -0.2}
                        },
                        {
                            'title': 'Apple and Microsoft partnership on AI technology',
                            'summary': 'Tech giants collaborate on artificial intelligence development',
                            'sentiment': {'polarity': 0.3}
                        }
                    ]
                }
            }
            news_network = analyzer.analyze_news_network(sample_news_data)
        
        # Reddit ë°ì´í„° ë¶„ì„ (ì‹œë®¬ë ˆì´ì…˜)
        reddit_network = analyzer.analyze_reddit_network({})
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = analyzer.generate_network_insights(news_network, reddit_network)
        
        return {
            'news_network': news_network,
            'reddit_network': reddit_network,
            'insights': insights
        }, None
        
    except Exception as e:
        import traceback
        error_details = f"{str(e)}\n{traceback.format_exc()}"
        return None, error_details

def show_network_analysis_page(news_data):
    """ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë©”ì¸ í˜ì´ì§€"""
    st.header("ğŸ•¸ï¸ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
    st.markdown("**ë‰´ìŠ¤ ë° SNS ë°ì´í„°ë¥¼ í™œìš©í•œ ì—”í‹°í‹° ê´€ê³„ ë¶„ì„**")
    
    # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìˆ˜í–‰
    with st.spinner("ğŸ”„ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìˆ˜í–‰ ì¤‘..."):
        network_data, error = analyze_network_data(news_data)
    
    if error:
        st.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹¤íŒ¨: {error}")
        return
    
    if not network_data:
        st.warning("âš ï¸ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë¶„ì„ ìš”ì•½
    show_network_summary(network_data)
    
    st.markdown("---")
    
    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸŒ ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬", "ğŸ“± ì†Œì…œ ë„¤íŠ¸ì›Œí¬", "ğŸ” ì¸ì‚¬ì´íŠ¸", "ğŸ“Š ë©”íŠ¸ë¦­"])
    
    with tab1:
        show_news_network_analysis(network_data['news_network'])
    
    with tab2:
        show_social_network_analysis(network_data['reddit_network'])
    
    with tab3:
        show_network_insights(network_data['insights'])
    
    with tab4:
        show_network_metrics(network_data)

def show_network_summary(network_data):
    """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìš”ì•½"""
    st.subheader("ğŸ“Š ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ìš”ì•½")
    
    news_network = network_data['news_network']
    reddit_network = network_data['reddit_network']
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "ë‰´ìŠ¤ ì—”í‹°í‹°",
            news_network.get('total_entities', 0),
            f"ê´€ê³„: {news_network.get('total_relationships', 0)}ê°œ"
        )
    
    with col2:
        communities_count = len(news_network.get('communities', []))
        st.metric(
            "ì—”í‹°í‹° ê·¸ë£¹",
            communities_count,
            "ì»¤ë®¤ë‹ˆí‹° ìˆ˜"
        )
    
    with col3:
        user_count = reddit_network['user_metrics'].get('nodes_count', 0)
        st.metric(
            "ì†Œì…œ ì‚¬ìš©ì",
            user_count,
            f"ìƒí˜¸ì‘ìš©: {reddit_network['user_metrics'].get('edges_count', 0)}ê°œ"
        )
    
    with col4:
        topic_count = reddit_network['topic_metrics'].get('nodes_count', 0)
        st.metric(
            "ì£¼ì œ ì—°ê´€ì„±",
            topic_count,
            f"ì—°ê²°: {reddit_network['topic_metrics'].get('edges_count', 0)}ê°œ"
        )

def show_news_network_analysis(news_network):
    """ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í‘œì‹œ"""
    st.subheader("ğŸŒ ë‰´ìŠ¤ ì—”í‹°í‹° ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
    
    G = news_network.get('graph')
    
    # ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒíƒœ í™•ì¸
    if not G:
        st.warning("âš ï¸ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ ì¶©ë¶„í•œ ì—”í‹°í‹°ë¥¼ ì°¾ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    
    if len(G.nodes()) == 0:
        st.warning("âš ï¸ ë„¤íŠ¸ì›Œí¬ì— ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ ê²½ì œ ê´€ë ¨ ì—”í‹°í‹°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ” ë””ë²„ê¹… ì •ë³´"):
            st.write("**ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼:**")
            st.write(f"- ì´ ì—”í‹°í‹°: {news_network.get('total_entities', 0)}ê°œ")
            st.write(f"- ì´ ê´€ê³„: {news_network.get('total_relationships', 0)}ê°œ")
            
            entity_mentions = news_network.get('entity_mentions', {})
            if entity_mentions:
                st.write("**ë°œê²¬ëœ ì—”í‹°í‹°:**")
                for entity, count in list(entity_mentions.items())[:10]:
                    st.write(f"  â€¢ {entity}: {count}íšŒ")
            else:
                st.write("**ë°œê²¬ëœ ì—”í‹°í‹°ê°€ ì—†ìŠµë‹ˆë‹¤.**")
        
        return
    
    # ë„¤íŠ¸ì›Œí¬ ê¸°ë³¸ ì •ë³´ í‘œì‹œ
    st.info(f"ğŸ“Š **ë„¤íŠ¸ì›Œí¬ ì •ë³´**: {len(G.nodes())}ê°œ ì—”í‹°í‹°, {len(G.edges())}ê°œ ê´€ê³„")
    
    # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
    st.markdown("#### ğŸ“ˆ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”")
    
    try:
        # NetworkX ê·¸ë˜í”„ë¥¼ Plotlyë¡œ ì‹œê°í™”
        fig = create_network_visualization(G, "ë‰´ìŠ¤ ì—”í‹°í‹° ë„¤íŠ¸ì›Œí¬")
        st.plotly_chart(fig, use_container_width=True)
    except Exception as e:
        st.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ì˜¤ë¥˜: {e}")
        st.info("ğŸ’¡ ë„¤íŠ¸ì›Œí¬ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì¤‘ìš”í•œ ì—”í‹°í‹° í‘œì‹œ
    st.markdown("#### ğŸ† ì¤‘ìš”í•œ ì—”í‹°í‹°")
    
    important_nodes = news_network.get('important_nodes', {})
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ¯ ì¤‘ì‹¬ì„± ê¸°ì¤€ ìƒìœ„ ì—”í‹°í‹°**")
        degree_nodes = important_nodes.get('by_degree', [])
        
        if degree_nodes:
            for i, (node, centrality) in enumerate(degree_nodes[:5], 1):
                mentions = G.nodes[node].get('mentions', 0)
                sentiment = G.nodes[node].get('avg_sentiment', 0)
                
                sentiment_emoji = "ğŸŸ¢" if sentiment > 0.1 else "ğŸ”´" if sentiment < -0.1 else "ğŸŸ¡"
                
                st.write(f"**{i}. {sentiment_emoji} {node}**")
                st.caption(f"ì¤‘ì‹¬ì„±: {centrality:.3f} | ì–¸ê¸‰: {mentions}íšŒ | ê°ì •: {sentiment:+.2f}")
        else:
            st.info("ì¤‘ì‹¬ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with col2:
        st.markdown("**ğŸ“¢ ì–¸ê¸‰ ë¹ˆë„ ê¸°ì¤€ ìƒìœ„ ì—”í‹°í‹°**")
        mention_nodes = important_nodes.get('by_mentions', [])
        
        if mention_nodes:
            for i, (node, mentions) in enumerate(mention_nodes[:5], 1):
                sentiment = G.nodes[node].get('avg_sentiment', 0)
                sentiment_emoji = "ğŸŸ¢" if sentiment > 0.1 else "ğŸ”´" if sentiment < -0.1 else "ğŸŸ¡"
                
                st.write(f"**{i}. {sentiment_emoji} {node}**")
                st.caption(f"ì–¸ê¸‰: {mentions}íšŒ | ê°ì •: {sentiment:+.2f}")
        else:
            st.info("ì–¸ê¸‰ ë¹ˆë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì£¼ìš” ê´€ê³„ í‘œì‹œ
    st.markdown("#### ğŸ”— ì£¼ìš” ì—”í‹°í‹° ê°„ ê´€ê³„")
    
    if len(G.edges()) > 0:
        # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ê´€ê³„ë“¤
        edges_by_weight = sorted(G.edges(data=True), 
                               key=lambda x: x[2].get('weight', 0), reverse=True)
        
        relationships_data = []
        for source, target, data in edges_by_weight[:10]:
            relationships_data.append({
                'ì—”í‹°í‹° 1': source,
                'ì—”í‹°í‹° 2': target,
                'ê´€ê³„ ê°•ë„': data.get('weight', 0),
                'ê´€ê³„ ìœ í˜•': data.get('relationship_type', 'unknown'),
                'ë§¥ë½ ì˜ˆì‹œ': data.get('contexts', [''])[0][:80] + "..." if data.get('contexts') else 'N/A'
            })
        
        if relationships_data:
            df = pd.DataFrame(relationships_data)
            st.dataframe(df, use_container_width=True)
        else:
            st.info("ê´€ê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ì—”í‹°í‹° ê°„ ê´€ê³„ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ì»¤ë®¤ë‹ˆí‹° ë¶„ì„
    st.markdown("#### ğŸ‘¥ ì—”í‹°í‹° ì»¤ë®¤ë‹ˆí‹°")
    
    communities = news_network.get('communities', [])
    if communities:
        for i, community in enumerate(communities, 1):
            if len(community) > 1:  # 2ê°œ ì´ìƒì˜ ë…¸ë“œê°€ ìˆëŠ” ì»¤ë®¤ë‹ˆí‹°ë§Œ í‘œì‹œ
                st.write(f"**ì»¤ë®¤ë‹ˆí‹° {i}**: {', '.join(community)}")
    else:
        st.info("ì»¤ë®¤ë‹ˆí‹°ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def show_social_network_analysis(reddit_network):
    """ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ í‘œì‹œ"""
    st.subheader("ğŸ“± ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
    
    user_network = reddit_network.get('user_network')
    topic_network = reddit_network.get('topic_network')
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ‘¥ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë„¤íŠ¸ì›Œí¬")
        
        if user_network and len(user_network.nodes()) > 0:
            # ì‚¬ìš©ì ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
            user_fig = create_network_visualization(user_network, "ì‚¬ìš©ì ìƒí˜¸ì‘ìš©")
            st.plotly_chart(user_fig, use_container_width=True)
            
            # ì‚¬ìš©ì ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­
            user_metrics = reddit_network.get('user_metrics', {})
            st.write(f"**ì‚¬ìš©ì ìˆ˜**: {user_metrics.get('nodes_count', 0)}")
            st.write(f"**ìƒí˜¸ì‘ìš© ìˆ˜**: {user_metrics.get('edges_count', 0)}")
            st.write(f"**ë„¤íŠ¸ì›Œí¬ ë°€ë„**: {user_metrics.get('density', 0):.3f}")
        else:
            st.info("ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with col2:
        st.markdown("#### ğŸ·ï¸ ì£¼ì œ ì—°ê´€ì„± ë„¤íŠ¸ì›Œí¬")
        
        if topic_network and len(topic_network.nodes()) > 0:
            # ì£¼ì œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
            topic_fig = create_network_visualization(topic_network, "ì£¼ì œ ì—°ê´€ì„±")
            st.plotly_chart(topic_fig, use_container_width=True)
            
            # ì£¼ì œ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­
            topic_metrics = reddit_network.get('topic_metrics', {})
            st.write(f"**ì£¼ì œ ìˆ˜**: {topic_metrics.get('nodes_count', 0)}")
            st.write(f"**ì—°ê´€ì„± ìˆ˜**: {topic_metrics.get('edges_count', 0)}")
            st.write(f"**ë„¤íŠ¸ì›Œí¬ ë°€ë„**: {topic_metrics.get('density', 0):.3f}")
        else:
            st.info("ì£¼ì œ ì—°ê´€ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def show_network_insights(insights):
    """ë„¤íŠ¸ì›Œí¬ ì¸ì‚¬ì´íŠ¸ í‘œì‹œ"""
    st.subheader("ğŸ” ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¸ì‚¬ì´íŠ¸")
    
    news_insights = insights.get('news_insights', {})
    combined_insights = insights.get('combined_insights', {})
    
    # ì£¼ìš” ë°œê²¬ì‚¬í•­
    st.markdown("#### ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­")
    
    key_findings = combined_insights.get('key_findings', [])
    if key_findings:
        for i, finding in enumerate(key_findings, 1):
            st.write(f"**{i}.** {finding}")
    else:
        st.info("ì£¼ìš” ë°œê²¬ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì¶”ì²œì‚¬í•­
    st.markdown("#### ğŸ’¡ ì¶”ì²œì‚¬í•­")
    
    recommendations = combined_insights.get('recommendations', [])
    if recommendations:
        for i, recommendation in enumerate(recommendations, 1):
            st.write(f"**{i}.** {recommendation}")
    else:
        st.info("ì¶”ì²œì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë„¤íŠ¸ì›Œí¬ íŠ¹ì„±
    st.markdown("#### ğŸ“Š ë„¤íŠ¸ì›Œí¬ íŠ¹ì„±")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸŒ ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬**")
        if news_insights:
            st.write(f"â€¢ ë„¤íŠ¸ì›Œí¬ í¬ê¸°: {news_insights.get('network_size', 'N/A')}")
            st.write(f"â€¢ ë„¤íŠ¸ì›Œí¬ ë°€ë„: {news_insights.get('network_density', 'N/A')}")
            st.write(f"â€¢ í´ëŸ¬ìŠ¤í„°ë§: {news_insights.get('clustering', 'N/A')}")
            st.write(f"â€¢ ì»¤ë®¤ë‹ˆí‹° ìˆ˜: {news_insights.get('communities_count', 0)}ê°œ")
    
    with col2:
        st.markdown("**ğŸ“± ì†Œì…œ ë„¤íŠ¸ì›Œí¬**")
        reddit_insights = insights.get('reddit_insights', {})
        if reddit_insights:
            st.write(f"â€¢ ì‚¬ìš©ì ë„¤íŠ¸ì›Œí¬: {reddit_insights.get('user_network_size', 0)}ê°œ ë…¸ë“œ")
            st.write(f"â€¢ ì£¼ì œ ë„¤íŠ¸ì›Œí¬: {reddit_insights.get('topic_network_size', 0)}ê°œ ë…¸ë“œ")
            st.write(f"â€¢ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©: {reddit_insights.get('user_interactions', 0)}ê°œ")
            st.write(f"â€¢ ì£¼ì œ ì—°ê´€ì„±: {reddit_insights.get('topic_correlations', 0)}ê°œ")
    
    # ì¤‘ì‹¬ì  ì—”í‹°í‹°
    st.markdown("#### ğŸ¯ ì¤‘ì‹¬ì  ì—”í‹°í‹°")
    
    most_central = news_insights.get('most_central_entities', [])
    most_mentioned = news_insights.get('most_mentioned_entities', [])
    sentiment_leaders = news_insights.get('sentiment_leaders', [])
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**ì¤‘ì‹¬ì„± ê¸°ì¤€**")
        for entity in most_central[:3]:
            st.write(f"â€¢ {entity}")
    
    with col2:
        st.markdown("**ì–¸ê¸‰ ë¹ˆë„ ê¸°ì¤€**")
        for entity in most_mentioned[:3]:
            st.write(f"â€¢ {entity}")
    
    with col3:
        st.markdown("**ê°ì • ì˜í–¥ë ¥ ê¸°ì¤€**")
        for entity in sentiment_leaders[:3]:
            st.write(f"â€¢ {entity}")

def show_network_metrics(network_data):
    """ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ìƒì„¸ í‘œì‹œ"""
    st.subheader("ğŸ“Š ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ìƒì„¸")
    
    news_network = network_data['news_network']
    reddit_network = network_data['reddit_network']
    
    # ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­
    st.markdown("#### ğŸŒ ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­")
    
    news_metrics = news_network.get('network_metrics', {})
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**ê¸°ë³¸ ë©”íŠ¸ë¦­**")
        st.write(f"ë…¸ë“œ ìˆ˜: {news_metrics.get('nodes_count', 0)}")
        st.write(f"ì—£ì§€ ìˆ˜: {news_metrics.get('edges_count', 0)}")
        st.write(f"ë°€ë„: {news_metrics.get('density', 0):.4f}")
        st.write(f"í‰ê·  í´ëŸ¬ìŠ¤í„°ë§: {news_metrics.get('average_clustering', 0):.4f}")
    
    with col2:
        st.markdown("**ì—°ê²°ì„± ë©”íŠ¸ë¦­**")
        if 'diameter' in news_metrics:
            st.write(f"ì§€ë¦„: {news_metrics['diameter']}")
            st.write(f"í‰ê·  ê²½ë¡œ ê¸¸ì´: {news_metrics['average_path_length']:.2f}")
        else:
            st.write(f"ì—°ê²° ì»´í¬ë„ŒíŠ¸: {news_metrics.get('connected_components', 'N/A')}")
            st.write(f"ìµœëŒ€ ì»´í¬ë„ŒíŠ¸: {news_metrics.get('largest_component_size', 'N/A')}")
    
    with col3:
        st.markdown("**ì¤‘ì‹¬ì„± ë¶„ì„**")
        top_degree = news_metrics.get('top_degree_centrality', [])
        if top_degree:
            st.write("**Degree Centrality Top 3:**")
            for node, centrality in top_degree[:3]:
                st.write(f"â€¢ {node}: {centrality:.3f}")
    
    # ì¤‘ì‹¬ì„± ë¹„êµ ì°¨íŠ¸
    if news_metrics.get('top_degree_centrality'):
        st.markdown("#### ğŸ“ˆ ì¤‘ì‹¬ì„± ë¹„êµ")
        
        degree_data = news_metrics.get('top_degree_centrality', [])[:10]
        betweenness_data = news_metrics.get('top_betweenness_centrality', [])[:10]
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        centrality_df = pd.DataFrame({
            'Entity': [item[0] for item in degree_data],
            'Degree_Centrality': [item[1] for item in degree_data]
        })
        
        # ì°¨íŠ¸ ìƒì„±
        fig = px.bar(
            centrality_df,
            x='Entity',
            y='Degree_Centrality',
            title="ì—”í‹°í‹°ë³„ Degree Centrality",
            labels={'Degree_Centrality': 'Degree Centrality', 'Entity': 'ì—”í‹°í‹°'}
        )
        fig.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
    
    # ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­
    st.markdown("#### ğŸ“± ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ì‚¬ìš©ì ë„¤íŠ¸ì›Œí¬**")
        user_metrics = reddit_network.get('user_metrics', {})
        st.write(f"ì‚¬ìš©ì ìˆ˜: {user_metrics.get('nodes_count', 0)}")
        st.write(f"ìƒí˜¸ì‘ìš© ìˆ˜: {user_metrics.get('edges_count', 0)}")
        st.write(f"ë°€ë„: {user_metrics.get('density', 0):.4f}")
        st.write(f"í´ëŸ¬ìŠ¤í„°ë§: {user_metrics.get('average_clustering', 0):.4f}")
    
    with col2:
        st.markdown("**ì£¼ì œ ë„¤íŠ¸ì›Œí¬**")
        topic_metrics = reddit_network.get('topic_metrics', {})
        st.write(f"ì£¼ì œ ìˆ˜: {topic_metrics.get('nodes_count', 0)}")
        st.write(f"ì—°ê´€ì„± ìˆ˜: {topic_metrics.get('edges_count', 0)}")
        st.write(f"ë°€ë„: {topic_metrics.get('density', 0):.4f}")
        st.write(f"í´ëŸ¬ìŠ¤í„°ë§: {topic_metrics.get('average_clustering', 0):.4f}")

def create_network_visualization(G, title):
    """NetworkX ê·¸ë˜í”„ë¥¼ Plotlyë¡œ ì‹œê°í™”"""
    
    if len(G.nodes()) == 0:
        # ë¹ˆ ê·¸ë˜í”„ ì²˜ë¦¬
        fig = go.Figure()
        fig.add_annotation(text="ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤", 
                          xref="paper", yref="paper",
                          x=0.5, y=0.5, showarrow=False)
        fig.update_layout(title=title)
        return fig
    
    # ë ˆì´ì•„ì›ƒ ê³„ì‚°
    pos = nx.spring_layout(G, k=1, iterations=50)
    
    # ì—£ì§€ ê·¸ë¦¬ê¸°
    edge_x = []
    edge_y = []
    edge_info = []
    
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])
        
        # ì—£ì§€ ì •ë³´
        weight = G[edge[0]][edge[1]].get('weight', 1)
        edge_info.append(f"{edge[0]} - {edge[1]}: {weight}")
    
    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=0.5, color='#888'),
        hoverinfo='none',
        mode='lines'
    )
    
    # ë…¸ë“œ ê·¸ë¦¬ê¸°
    node_x = []
    node_y = []
    node_text = []
    node_info = []
    node_size = []
    node_color = []
    
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_text.append(node)
        
        # ë…¸ë“œ ì •ë³´
        mentions = G.nodes[node].get('mentions', 0)
        sentiment = G.nodes[node].get('avg_sentiment', 0)
        degree = G.degree(node)
        
        node_info.append(f"{node}<br>ì–¸ê¸‰: {mentions}íšŒ<br>ê°ì •: {sentiment:.2f}<br>ì—°ê²°: {degree}ê°œ")
        
        # ë…¸ë“œ í¬ê¸° (degree ê¸°ë°˜)
        node_size.append(max(10, degree * 3))
        
        # ë…¸ë“œ ìƒ‰ìƒ (ê°ì • ê¸°ë°˜)
        if sentiment > 0.1:
            node_color.append('green')
        elif sentiment < -0.1:
            node_color.append('red')
        else:
            node_color.append('blue')
    
    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        hoverinfo='text',
        text=node_text,
        hovertext=node_info,
        textposition="middle center",
        marker=dict(
            size=node_size,
            color=node_color,
            line=dict(width=2, color='white')
        )
    )
    
    # ê·¸ë˜í”„ ìƒì„±
    fig = go.Figure(data=[edge_trace, node_trace],
                   layout=go.Layout(
                       title=dict(text=title, font=dict(size=16)),
                       showlegend=False,
                       hovermode='closest',
                       margin=dict(b=20,l=5,r=5,t=40),
                       annotations=[ dict(
                           text="ë…¸ë“œ í¬ê¸°: ì—°ê²° ìˆ˜ | ìƒ‰ìƒ: ê°ì • (ë…¹ìƒ‰: ê¸ì •, ë¹¨ê°•: ë¶€ì •, íŒŒë‘: ì¤‘ë¦½)",
                           showarrow=False,
                           xref="paper", yref="paper",
                           x=0.005, y=-0.002,
                           xanchor="left", yanchor="bottom",
                           font=dict(size=10)
                       )],
                       xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                       yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
                   ))
    
    return fig

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°
    sample_news_data = {
        'categories': {
            'financial': [
                {
                    'title': 'Federal Reserve raises interest rates',
                    'summary': 'Jerome Powell announced the decision',
                    'sentiment': {'polarity': -0.2}
                }
            ]
        }
    }
    
    show_network_analysis_page(sample_news_data)
