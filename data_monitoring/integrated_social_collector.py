#!/usr/bin/env python3
"""
í†µí•© ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘ê¸°
Twitter API, ë¬´ë£Œ ëŒ€ì•ˆ, ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ í†µí•© ê´€ë¦¬
"""

import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
import os
import sys

# ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from twitter_api_collector import TwitterAPICollector
from free_twitter_alternatives import FreeTwitterAlternatives

class IntegratedSocialCollector:
    """í†µí•© ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # Twitter API ìˆ˜ì§‘ê¸° (ìœ ë£Œ)
        self.twitter_collector = TwitterAPICollector()
        
        # ë¬´ë£Œ ëŒ€ì•ˆ ìˆ˜ì§‘ê¸°
        self.free_collector = FreeTwitterAlternatives()
        
        # API í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        self.has_twitter_api = bool(os.getenv('TWITTER_BEARER_TOKEN'))
        
        self.logger.info(f"âœ… í†µí•© ì†Œì…œ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ (Twitter API: {'ì‚¬ìš© ê°€ëŠ¥' if self.has_twitter_api else 'ì‚¬ìš© ë¶ˆê°€'})")
    
    def collect_social_data_for_network_analysis(self, data_source: str = "auto", max_items: int = 100) -> List[str]:
        """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ìš© ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘"""
        
        self.logger.info(f"ğŸ” ë„¤íŠ¸ì›Œí¬ ë¶„ì„ìš© ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ì†ŒìŠ¤: {data_source})")
        
        texts = []
        
        try:
            if data_source == "auto":
                # ìë™ ì„ íƒ: API í‚¤ê°€ ìˆìœ¼ë©´ Twitter API, ì—†ìœ¼ë©´ ë¬´ë£Œ ëŒ€ì•ˆ
                if self.has_twitter_api:
                    data_source = "twitter_api"
                else:
                    data_source = "free_alternatives"
            
            if data_source == "twitter_api" and self.has_twitter_api:
                # Twitter API ì‚¬ìš©
                twitter_data = self.twitter_collector.collect_economic_tweets(max_items)
                texts = self._extract_texts_from_twitter_data(twitter_data)
                
            elif data_source == "free_alternatives":
                # ë¬´ë£Œ ëŒ€ì•ˆ ì‚¬ìš©
                alternative_data = self.free_collector.collect_alternative_social_data()
                texts = self._extract_texts_from_alternative_data(alternative_data)
                
            elif data_source == "reddit_only":
                # Redditë§Œ ì‚¬ìš©
                reddit_data = self.free_collector._collect_reddit_data()
                if reddit_data:
                    texts = [f"{post['title']} {post['text']}" for post in reddit_data['posts']]
                
            elif data_source == "simulation":
                # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì‚¬ìš©
                simulation_data = self.free_collector._generate_realistic_simulation()
                texts = [post['text'] for post in simulation_data['posts']]
                
            else:
                # ê¸°ë³¸ê°’: ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
                self.logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° ì†ŒìŠ¤: {data_source}, ì‹œë®¬ë ˆì´ì…˜ ì‚¬ìš©")
                simulation_data = self.free_collector._generate_realistic_simulation()
                texts = [post['text'] for post in simulation_data['posts']]
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            texts = self._clean_texts(texts)
            
            self.logger.info(f"âœ… ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            return texts
            
        except Exception as e:
            self.logger.error(f"âŒ ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            # ë°±ì—…: ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì‚¬ìš©
            simulation_data = self.free_collector._generate_realistic_simulation()
            return [post['text'] for post in simulation_data['posts']]
    
    def _extract_texts_from_twitter_data(self, twitter_data: Dict[str, Any]) -> List[str]:
        """Twitter API ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = []
        
        if twitter_data.get('status') == 'success':
            for tweet in twitter_data.get('tweets', []):
                text = tweet.get('text', '').strip()
                if text and len(text) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                    texts.append(text)
        
        return texts
    
    def _extract_texts_from_alternative_data(self, alternative_data: Dict[str, Any]) -> List[str]:
        """ë¬´ë£Œ ëŒ€ì•ˆ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = []
        
        for source_name, source_data in alternative_data.get('sources', {}).items():
            if source_name == 'reddit':
                for post in source_data.get('posts', []):
                    title = post.get('title', '').strip()
                    content = post.get('text', '').strip()
                    combined_text = f"{title} {content}".strip()
                    if combined_text and len(combined_text) > 10:
                        texts.append(combined_text)
            
            elif source_name == 'nitter':
                for post in source_data.get('posts', []):
                    text = post.get('text', '').strip()
                    if text and len(text) > 10:
                        texts.append(text)
            
            elif source_name == 'news_comments':
                for comment in source_data.get('comments', []):
                    text = comment.get('text', '').strip()
                    if text and len(text) > 10:
                        texts.append(text)
            
            elif source_name == 'forums':
                for post in source_data.get('posts', []):
                    title = post.get('title', '').strip()
                    content = post.get('content', '').strip()
                    combined_text = f"{title} {content}".strip()
                    if combined_text and len(combined_text) > 10:
                        texts.append(combined_text)
            
            elif source_name == 'simulation':
                for post in source_data.get('posts', []):
                    text = post.get('text', '').strip()
                    if text and len(text) > 10:
                        texts.append(text)
        
        return texts
    
    def _clean_texts(self, texts: List[str]) -> List[str]:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        cleaned_texts = []
        
        for text in texts:
            # ê¸°ë³¸ ì •ì œ
            text = text.strip()
            
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ í…ìŠ¤íŠ¸ í•„í„°
            if 10 <= len(text) <= 1000:
                # URL ì œê±°
                import re
                text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
                
                # í•´ì‹œíƒœê·¸ ì •ë¦¬ (ì œê±°í•˜ì§€ ì•Šê³  ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬)
                text = re.sub(r'#(\w+)', r'\1', text)
                
                # ë©˜ì…˜ ì œê±°
                text = re.sub(r'@\w+', '', text)
                
                # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                text = re.sub(r'\s+', ' ', text).strip()
                
                if text:
                    cleaned_texts.append(text)
        
        return cleaned_texts
    
    def get_available_data_sources(self) -> Dict[str, Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ ë°˜í™˜"""
        
        sources = {
            "auto": {
                "name": "ìë™ ì„ íƒ",
                "description": "API í‚¤ ìœ ë¬´ì— ë”°ë¼ ìë™ ì„ íƒ",
                "cost": "ë¬´ë£Œ ë˜ëŠ” ìœ ë£Œ",
                "reliability": "ë†’ìŒ",
                "available": True
            },
            "twitter_api": {
                "name": "Twitter API",
                "description": "ê³µì‹ Twitter API v2",
                "cost": "$100+/ì›”",
                "reliability": "ë§¤ìš° ë†’ìŒ",
                "available": self.has_twitter_api,
                "note": "API í‚¤ í•„ìš”" if not self.has_twitter_api else "ì‚¬ìš© ê°€ëŠ¥"
            },
            "free_alternatives": {
                "name": "ë¬´ë£Œ ëŒ€ì•ˆ",
                "description": "Reddit + Nitter + ë‰´ìŠ¤ ëŒ“ê¸€",
                "cost": "ë¬´ë£Œ",
                "reliability": "ë³´í†µ",
                "available": True
            },
            "reddit_only": {
                "name": "Reddit ì „ìš©",
                "description": "Reddit ê²½ì œ ì„œë¸Œë ˆë”§",
                "cost": "ë¬´ë£Œ",
                "reliability": "ë†’ìŒ",
                "available": True
            },
            "simulation": {
                "name": "ì‹œë®¬ë ˆì´ì…˜",
                "description": "í˜„ì‹¤ì ì¸ ìƒ˜í”Œ ë°ì´í„°",
                "cost": "ë¬´ë£Œ",
                "reliability": "ë³´í†µ",
                "available": True,
                "note": "í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ìš©"
            }
        }
        
        return sources
    
    def test_all_sources(self) -> Dict[str, Any]:
        """ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸"""
        
        self.logger.info("ğŸ§ª ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = {}
        sources = self.get_available_data_sources()
        
        for source_id, source_info in sources.items():
            if not source_info['available']:
                test_results[source_id] = {
                    'status': 'unavailable',
                    'reason': source_info.get('note', 'Not available')
                }
                continue
            
            try:
                self.logger.info(f"ğŸ” {source_info['name']} í…ŒìŠ¤íŠ¸ ì¤‘...")
                
                texts = self.collect_social_data_for_network_analysis(
                    data_source=source_id, 
                    max_items=10
                )
                
                test_results[source_id] = {
                    'status': 'success',
                    'text_count': len(texts),
                    'sample_text': texts[0] if texts else None,
                    'cost': source_info['cost'],
                    'reliability': source_info['reliability']
                }
                
                self.logger.info(f"âœ… {source_info['name']}: {len(texts)}ê°œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘")
                
            except Exception as e:
                test_results[source_id] = {
                    'status': 'error',
                    'error': str(e),
                    'cost': source_info['cost'],
                    'reliability': source_info['reliability']
                }
                
                self.logger.error(f"âŒ {source_info['name']} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        return test_results

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    collector = IntegratedSocialCollector()
    
    print("ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì†ŒìŠ¤:")
    sources = collector.get_available_data_sources()
    
    for source_id, info in sources.items():
        status = "âœ…" if info['available'] else "âŒ"
        print(f"{status} {info['name']}: {info['description']} ({info['cost']})")
        if 'note' in info:
            print(f"   ğŸ“ {info['note']}")
    
    print("\nğŸ§ª ì „ì²´ ì†ŒìŠ¤ í…ŒìŠ¤íŠ¸:")
    test_results = collector.test_all_sources()
    
    for source_id, result in test_results.items():
        if result['status'] == 'success':
            print(f"âœ… {source_id}: {result['text_count']}ê°œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì„±ê³µ")
        elif result['status'] == 'unavailable':
            print(f"âš ï¸ {source_id}: {result['reason']}")
        else:
            print(f"âŒ {source_id}: {result['error']}")
    
    print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    if collector.has_twitter_api:
        print("- Twitter API í‚¤ê°€ ìˆìœ¼ë¯€ë¡œ 'twitter_api' ë˜ëŠ” 'auto' ì‚¬ìš© ê¶Œì¥")
    else:
        print("- Twitter API í‚¤ê°€ ì—†ìœ¼ë¯€ë¡œ 'free_alternatives' ë˜ëŠ” 'reddit_only' ì‚¬ìš© ê¶Œì¥")
        print("- í…ŒìŠ¤íŠ¸ ëª©ì ì´ë¼ë©´ 'simulation' ì‚¬ìš© ê°€ëŠ¥")
