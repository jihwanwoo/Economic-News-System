#!/usr/bin/env python3
"""
ê°œì„ ëœ ê²½ì œ ê°œë… ê¸°ë°˜ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ëª¨ë“ˆ
SNS ë° ëŒ“ê¸€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê²½ì œ ê°œë…ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„
"""

import networkx as nx
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter, defaultdict
import re
from textblob import TextBlob
import json

class EnhancedEconomicNetworkAnalyzer:
    """ê°œì„ ëœ ê²½ì œ ê°œë… ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # 16ê°œ ì£¼ìš” ê²½ì œ ì¹´í…Œê³ ë¦¬ì™€ ì„¸ë¶€ ê°œë…ë“¤
        self.economic_concepts = {
            # 1. í†µí™”ì •ì±… (Monetary Policy)
            'monetary_policy': {
                'main_concepts': ['í†µí™”ì •ì±…', 'ê¸ˆë¦¬ì •ì±…', 'ê¸°ì¤€ê¸ˆë¦¬', 'ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬', 'Fed Rate'],
                'related_terms': [
                    'ê¸ˆë¦¬ì¸ìƒ', 'ê¸ˆë¦¬ì¸í•˜', 'ê¸ˆë¦¬ë™ê²°', 'ì–‘ì ì™„í™”', 'QE', 'Quantitative Easing',
                    'í…Œì´í¼ë§', 'Tapering', 'ê¸´ì¶•ì •ì±…', 'ì™„í™”ì •ì±…', 'ì¤‘ì•™ì€í–‰', 'Federal Reserve',
                    'Fed', 'ECB', 'BOJ', 'PBOC', 'í†µí™”ê³µê¸‰', 'Money Supply', 'FOMC'
                ],
                'weight': 1.0
            },
            
            # 2. ì¸í”Œë ˆì´ì…˜ (Inflation)
            'inflation': {
                'main_concepts': ['ì¸í”Œë ˆì´ì…˜', 'Inflation', 'ë¬¼ê°€ìƒìŠ¹', 'CPI', 'PCE'],
                'related_terms': [
                    'ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜', 'Consumer Price Index', 'ê·¼ì›ì¸í”Œë ˆì´ì…˜', 'Core Inflation',
                    'ë””í”Œë ˆì´ì…˜', 'Deflation', 'ìŠ¤íƒœê·¸í”Œë ˆì´ì…˜', 'Stagflation', 'ë¬¼ê°€ì•ˆì •',
                    'ì¸í”Œë ˆì´ì…˜ íƒ€ê²Ÿ', 'Inflation Target', 'ë¬¼ê°€ì••ë ¥', 'Price Pressure'
                ],
                'weight': 1.0
            },
            
            # 3. ì£¼ì‹ì‹œì¥ (Stock Market)
            'stock_market': {
                'main_concepts': ['ì£¼ì‹ì‹œì¥', 'Stock Market', 'ì¦ì‹œ', 'Equity Market'],
                'related_terms': [
                    'S&P 500', 'NASDAQ', 'Dow Jones', 'KOSPI', 'KOSDAQ', 'ìƒìŠ¹ì¥', 'í•˜ë½ì¥',
                    'Bull Market', 'Bear Market', 'ë³€ë™ì„±', 'Volatility', 'VIX', 'ê³µí¬ì§€ìˆ˜',
                    'ì‹œê°€ì´ì•¡', 'Market Cap', 'ê±°ë˜ëŸ‰', 'Volume', 'P/E Ratio', 'PER'
                ],
                'weight': 1.0
            },
            
            # 4. ê¸°ì—…ì‹¤ì  (Corporate Performance)
            'corporate_performance': {
                'main_concepts': ['ê¸°ì—…ì‹¤ì ', 'ì‹¤ì ë°œí‘œ', 'Earnings', 'Corporate Results'],
                'related_terms': [
                    'ë§¤ì¶œ', 'Revenue', 'ìˆœì´ìµ', 'Net Income', 'ì˜ì—…ì´ìµ', 'Operating Income',
                    'EPS', 'Earnings Per Share', 'ê°€ì´ë˜ìŠ¤', 'Guidance', 'ì‹¤ì ì „ë§',
                    'ë¶„ê¸°ì‹¤ì ', 'Quarterly Results', 'ì—°ê°„ì‹¤ì ', 'Annual Results'
                ],
                'weight': 0.9
            },
            
            # 5. ê¸°ìˆ ì£¼ (Technology Stocks)
            'technology': {
                'main_concepts': ['ê¸°ìˆ ì£¼', 'Tech Stocks', 'í…Œí¬ì£¼', 'Technology Sector'],
                'related_terms': [
                    'Apple', 'Microsoft', 'Google', 'Amazon', 'Tesla', 'Meta', 'Netflix',
                    'NVIDIA', 'Intel', 'AMD', 'ë°˜ë„ì²´', 'Semiconductor', 'AI', 'ì¸ê³µì§€ëŠ¥',
                    'í´ë¼ìš°ë“œ', 'Cloud Computing', 'ì „ê¸°ì°¨', 'EV', 'Electric Vehicle'
                ],
                'weight': 0.9
            },
            
            # 6. ê¸ˆìœµì„¹í„° (Financial Sector)
            'financial_sector': {
                'main_concepts': ['ê¸ˆìœµì„¹í„°', 'Financial Sector', 'ì€í–‰ì£¼', 'Banking'],
                'related_terms': [
                    'JPMorgan', 'Goldman Sachs', 'Bank of America', 'Wells Fargo',
                    'ìˆœì´ìë§ˆì§„', 'NIM', 'ëŒ€ì¶œ', 'Lending', 'ì‹ ìš©ìœ„í—˜', 'Credit Risk',
                    'ìë³¸ë¹„ìœ¨', 'Capital Ratio', 'ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸', 'Stress Test'
                ],
                'weight': 0.8
            },
            
            # 7. ì—ë„ˆì§€ (Energy)
            'energy': {
                'main_concepts': ['ì—ë„ˆì§€', 'Energy', 'ì›ìœ ', 'Oil', 'ì²œì—°ê°€ìŠ¤', 'Natural Gas'],
                'related_terms': [
                    'WTI', 'Brent', 'OPEC', 'ì…°ì¼ì˜¤ì¼', 'Shale Oil', 'ì •ìœ ', 'Refining',
                    'ì‹ ì¬ìƒì—ë„ˆì§€', 'Renewable Energy', 'íƒœì–‘ê´‘', 'Solar', 'í’ë ¥', 'Wind',
                    'ExxonMobil', 'Chevron', 'Shell', 'BP'
                ],
                'weight': 0.8
            },
            
            # 8. ë¶€ë™ì‚° (Real Estate)
            'real_estate': {
                'main_concepts': ['ë¶€ë™ì‚°', 'Real Estate', 'ì£¼íƒì‹œì¥', 'Housing Market'],
                'related_terms': [
                    'ì£¼íƒê°€ê²©', 'Home Prices', 'ëª¨ê¸°ì§€', 'Mortgage', 'ì£¼íƒë‹´ë³´ëŒ€ì¶œ',
                    'ë¶€ë™ì‚° íˆ¬ì', 'Real Estate Investment', 'REIT', 'ìƒì—…ìš© ë¶€ë™ì‚°',
                    'Commercial Real Estate', 'ì£¼íƒ íŒë§¤', 'Home Sales'
                ],
                'weight': 0.8
            },
            
            # 9. êµ­ì œë¬´ì—­ (International Trade)
            'international_trade': {
                'main_concepts': ['êµ­ì œë¬´ì—­', 'International Trade', 'ë¬´ì—­ì „ìŸ', 'Trade War'],
                'related_terms': [
                    'ê´€ì„¸', 'Tariff', 'ìˆ˜ì¶œ', 'Export', 'ìˆ˜ì…', 'Import', 'ë¬´ì—­ìˆ˜ì§€', 'Trade Balance',
                    'ê³µê¸‰ë§', 'Supply Chain', 'ê¸€ë¡œë²Œí™”', 'Globalization', 'WTO',
                    'ë¯¸ì¤‘ë¬´ì—­', 'US-China Trade', 'ë¸Œë ‰ì‹œíŠ¸', 'Brexit'
                ],
                'weight': 0.8
            },
            
            # 10. ì•”í˜¸í™”í (Cryptocurrency)
            'cryptocurrency': {
                'main_concepts': ['ì•”í˜¸í™”í', 'Cryptocurrency', 'ë¹„íŠ¸ì½”ì¸', 'Bitcoin'],
                'related_terms': [
                    'Ethereum', 'ì´ë”ë¦¬ì›€', 'ë¸”ë¡ì²´ì¸', 'Blockchain', 'DeFi', 'NFT',
                    'ë””ì§€í„¸ ìì‚°', 'Digital Asset', 'ê°€ìƒí™”í', 'Virtual Currency',
                    'CBDC', 'ì¤‘ì•™ì€í–‰ ë””ì§€í„¸í™”í', 'Stablecoin', 'ìŠ¤í…Œì´ë¸”ì½”ì¸'
                ],
                'weight': 0.7
            },
            
            # 11. ESG (Environmental, Social, Governance)
            'esg': {
                'main_concepts': ['ESG', 'ì§€ì†ê°€ëŠ¥ì„±', 'Sustainability', 'ì¹œí™˜ê²½'],
                'related_terms': [
                    'íƒ„ì†Œì¤‘ë¦½', 'Carbon Neutral', 'ê¸°í›„ë³€í™”', 'Climate Change',
                    'ê·¸ë¦°ì—ë„ˆì§€', 'Green Energy', 'ì‚¬íšŒì  ì±…ì„', 'Social Responsibility',
                    'ì§€ë°°êµ¬ì¡°', 'Governance', 'ì§€ì†ê°€ëŠ¥ íˆ¬ì', 'Sustainable Investment'
                ],
                'weight': 0.7
            },
            
            # 12. ê³ ìš©ì‹œì¥ (Labor Market)
            'labor_market': {
                'main_concepts': ['ê³ ìš©ì‹œì¥', 'Labor Market', 'ì‹¤ì—…ë¥ ', 'Unemployment Rate'],
                'related_terms': [
                    'ë¹„ë†ì—… ê³ ìš©', 'Non-farm Payroll', 'êµ¬ì¸', 'Job Opening', 'ì„ê¸ˆìƒìŠ¹',
                    'Wage Growth', 'ë…¸ë™ì°¸ì—¬ìœ¨', 'Labor Participation Rate',
                    'êµ¬ì§ê¸‰ì—¬', 'Unemployment Benefits', 'ì¼ìë¦¬ ì°½ì¶œ', 'Job Creation'
                ],
                'weight': 0.8
            },
            
            # 13. ì†Œë¹„ (Consumer Spending)
            'consumer_spending': {
                'main_concepts': ['ì†Œë¹„', 'Consumer Spending', 'ì†Œë§¤íŒë§¤', 'Retail Sales'],
                'related_terms': [
                    'ì†Œë¹„ì ì‹ ë¢°', 'Consumer Confidence', 'ê°œì¸ì†Œë¹„', 'Personal Consumption',
                    'ì†Œë¹„ì‹¬ë¦¬', 'Consumer Sentiment', 'ê°€ê³„ì†Œë“', 'Household Income',
                    'ì €ì¶•ë¥ ', 'Savings Rate', 'ì†Œë¹„íŒ¨í„´', 'Consumption Pattern'
                ],
                'weight': 0.8
            },
            
            # 14. ì •ë¶€ì •ì±… (Government Policy)
            'government_policy': {
                'main_concepts': ['ì •ë¶€ì •ì±…', 'Government Policy', 'ì¬ì •ì •ì±…', 'Fiscal Policy'],
                'related_terms': [
                    'ì •ë¶€ì§€ì¶œ', 'Government Spending', 'ì„¸ê¸ˆ', 'Tax', 'ì„¸ìœ¨', 'Tax Rate',
                    'ë¶€ì±„', 'Debt', 'ì ì', 'Deficit', 'ì˜ˆì‚°', 'Budget', 'ë¶€ì–‘ì±…', 'Stimulus',
                    'ì¸í”„ë¼', 'Infrastructure', 'ê·œì œ', 'Regulation'
                ],
                'weight': 0.8
            },
            
            # 15. ì§€ì •í•™ì  ë¦¬ìŠ¤í¬ (Geopolitical Risk)
            'geopolitical_risk': {
                'main_concepts': ['ì§€ì •í•™ì  ë¦¬ìŠ¤í¬', 'Geopolitical Risk', 'êµ­ì œì •ì¹˜', 'International Politics'],
                'related_terms': [
                    'ì „ìŸ', 'War', 'ë¶„ìŸ', 'Conflict', 'ì œì¬', 'Sanctions', 'ì™¸êµ', 'Diplomacy',
                    'ì•ˆë³´', 'Security', 'í…ŒëŸ¬', 'Terrorism', 'ì •ì¹˜ì  ë¶ˆì•ˆ', 'Political Instability',
                    'ì„ ê±°', 'Election', 'ì •ê¶Œêµì²´', 'Regime Change'
                ],
                'weight': 0.7
            },
            
            # 16. ì‹œì¥ì‹¬ë¦¬ (Market Sentiment)
            'market_sentiment': {
                'main_concepts': ['ì‹œì¥ì‹¬ë¦¬', 'Market Sentiment', 'íˆ¬ìì‹¬ë¦¬', 'Investor Sentiment'],
                'related_terms': [
                    'ê³µí¬', 'Fear', 'íƒìš•', 'Greed', 'ë‚™ê´€', 'Optimism', 'ë¹„ê´€', 'Pessimism',
                    'ìœ„í—˜íšŒí”¼', 'Risk Aversion', 'ìœ„í—˜ì„ í˜¸', 'Risk Appetite',
                    'ì‹œì¥ ë¶„ìœ„ê¸°', 'Market Mood', 'íˆ¬ìì ì‹ ë¢°', 'Investor Confidence'
                ],
                'weight': 0.7
            }
        }
        
        # ê´€ê³„ ìœ í˜•ë³„ ê°€ì¤‘ì¹˜
        self.relationship_weights = {
            'strong_correlation': 1.0,      # ê°•í•œ ìƒê´€ê´€ê³„
            'moderate_correlation': 0.7,    # ë³´í†µ ìƒê´€ê´€ê³„
            'weak_correlation': 0.4,        # ì•½í•œ ìƒê´€ê´€ê³„
            'causal_relationship': 0.9,     # ì¸ê³¼ê´€ê³„
            'inverse_relationship': 0.8,    # ì—­ìƒê´€ê´€ê³„
            'temporal_relationship': 0.6,   # ì‹œê°„ì  ê´€ê³„
            'mentioned_together': 0.3       # ë‹¨ìˆœ ë™ì‹œ ì–¸ê¸‰
        }
        
        self.logger.info("âœ… ê°œì„ ëœ ê²½ì œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_economic_concepts(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê²½ì œ ê°œë… ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        text_lower = text.lower()
        # HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text_clean = re.sub(r'<[^>]+>', '', text)
        text_clean = re.sub(r'[^\w\sê°€-í£]', ' ', text_clean)
        text_clean_lower = text_clean.lower()
        
        found_concepts = {}
        concept_scores = {}
        
        for category, concept_data in self.economic_concepts.items():
            category_score = 0
            found_terms = []
            
            # ì£¼ìš” ê°œë… ê²€ìƒ‰ (ë†’ì€ ê°€ì¤‘ì¹˜)
            for main_concept in concept_data['main_concepts']:
                if main_concept.lower() in text_clean_lower:
                    category_score += 2.0 * concept_data['weight']
                    found_terms.append(main_concept)
            
            # ê´€ë ¨ ìš©ì–´ ê²€ìƒ‰ (ë‚®ì€ ê°€ì¤‘ì¹˜)
            for related_term in concept_data['related_terms']:
                if related_term.lower() in text_clean_lower:
                    category_score += 1.0 * concept_data['weight']
                    found_terms.append(related_term)
            
            if category_score > 0:
                found_concepts[category] = {
                    'score': category_score,
                    'terms': list(set(found_terms)),
                    'weight': concept_data['weight']
                }
                concept_scores[category] = category_score
        
        return found_concepts, concept_scores
    
    def analyze_concept_relationships(self, texts: List[str]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì—ì„œ ê²½ì œ ê°œë… ê°„ ê´€ê³„ ë¶„ì„"""
        self.logger.info("ğŸ” ê²½ì œ ê°œë… ê´€ê³„ ë¶„ì„ ì‹œì‘")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ ê°œë… ì¶”ì¶œ
        all_concepts = {}
        concept_cooccurrence = defaultdict(lambda: defaultdict(int))
        concept_sentiments = defaultdict(list)
        
        for text in texts:
            concepts, scores = self.extract_economic_concepts(text)
            
            # ê°ì • ë¶„ì„
            try:
                sentiment = TextBlob(text).sentiment.polarity
            except:
                sentiment = 0.0
            
            # ê°œë…ë³„ ì ìˆ˜ ëˆ„ì 
            for concept, data in concepts.items():
                if concept not in all_concepts:
                    all_concepts[concept] = {
                        'total_score': 0,
                        'mention_count': 0,
                        'terms': set(),
                        'weight': data['weight']
                    }
                
                all_concepts[concept]['total_score'] += data['score']
                all_concepts[concept]['mention_count'] += 1
                all_concepts[concept]['terms'].update(data['terms'])
                concept_sentiments[concept].append(sentiment)
            
            # ë™ì‹œ ì¶œí˜„ ê´€ê³„ ê³„ì‚°
            concept_list = list(concepts.keys())
            for i, concept1 in enumerate(concept_list):
                for concept2 in concept_list[i+1:]:
                    # ìƒí˜¸ ê°€ì¤‘ì¹˜ ì ìš©
                    weight = (concepts[concept1]['score'] * concepts[concept2]['score']) ** 0.5
                    concept_cooccurrence[concept1][concept2] += weight
                    concept_cooccurrence[concept2][concept1] += weight
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        # ë…¸ë“œ ì¶”ê°€ (ê°œë…ë“¤)
        for concept, data in all_concepts.items():
            avg_sentiment = np.mean(concept_sentiments[concept]) if concept_sentiments[concept] else 0
            
            G.add_node(concept, 
                      score=data['total_score'],
                      mentions=data['mention_count'],
                      terms=list(data['terms']),
                      weight=data['weight'],
                      sentiment=avg_sentiment,
                      size=min(data['total_score'] * 10, 100))  # ì‹œê°í™”ìš© í¬ê¸°
        
        # ì—£ì§€ ì¶”ê°€ (ê´€ê³„ë“¤)
        edges_added = 0
        for concept1, connections in concept_cooccurrence.items():
            for concept2, weight in connections.items():
                if weight > 0.5:  # ì„ê³„ê°’ ì´ìƒì˜ ê´€ê³„ë§Œ í¬í•¨
                    # ê´€ê³„ ìœ í˜• ê²°ì •
                    relationship_type = self._determine_relationship_type_advanced(
                        concept1, concept2, weight
                    )
                    
                    # ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜
                    normalized_weight = min(weight / 10.0, 1.0)
                    
                    G.add_edge(concept1, concept2,
                              weight=normalized_weight,
                              relationship_type=relationship_type,
                              strength=weight)
                    edges_added += 1
        
        # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._calculate_network_metrics(G)
        
        self.logger.info(f"âœ… ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ: {len(G.nodes())}ê°œ ë…¸ë“œ, {len(G.edges())}ê°œ ì—£ì§€")
        
        return {
            'graph': G,
            'concepts': all_concepts,
            'metrics': metrics,
            'node_count': len(G.nodes()),
            'edge_count': len(G.edges()),
            'concept_sentiments': dict(concept_sentiments)
        }
    
    def _determine_relationship_type_advanced(self, concept1: str, concept2: str, weight: float) -> str:
        """ê³ ê¸‰ ê´€ê³„ ìœ í˜• ê²°ì •"""
        # íŠ¹ì • ê°œë… ìŒì— ëŒ€í•œ ê´€ê³„ ìœ í˜• ì •ì˜
        strong_correlations = {
            ('monetary_policy', 'inflation'): 'causal_relationship',
            ('inflation', 'stock_market'): 'inverse_relationship',
            ('technology', 'stock_market'): 'strong_correlation',
            ('geopolitical_risk', 'market_sentiment'): 'causal_relationship',
            ('labor_market', 'consumer_spending'): 'strong_correlation',
            ('government_policy', 'market_sentiment'): 'moderate_correlation',
            ('energy', 'inflation'): 'strong_correlation',
            ('real_estate', 'monetary_policy'): 'strong_correlation',
            ('cryptocurrency', 'market_sentiment'): 'strong_correlation',
            ('esg', 'technology'): 'moderate_correlation'
        }
        
        # ìˆœì„œ ë¬´ê´€í•˜ê²Œ ê²€ìƒ‰
        pair1 = (concept1, concept2)
        pair2 = (concept2, concept1)
        
        if pair1 in strong_correlations:
            return strong_correlations[pair1]
        elif pair2 in strong_correlations:
            return strong_correlations[pair2]
        
        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê´€ê³„ ìœ í˜• ê²°ì •
        if weight > 5.0:
            return 'strong_correlation'
        elif weight > 3.0:
            return 'moderate_correlation'
        elif weight > 1.0:
            return 'weak_correlation'
        else:
            return 'mentioned_together'
    
    def _calculate_network_metrics(self, G: nx.Graph) -> Dict[str, Any]:
        """ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if len(G.nodes()) == 0:
            return {}
        
        metrics = {}
        
        try:
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            metrics['density'] = nx.density(G)
            metrics['average_clustering'] = nx.average_clustering(G)
            
            # ì¤‘ì‹¬ì„± ì§€í‘œ
            degree_centrality = nx.degree_centrality(G)
            betweenness_centrality = nx.betweenness_centrality(G)
            closeness_centrality = nx.closeness_centrality(G)
            
            metrics['centrality'] = {
                'degree': degree_centrality,
                'betweenness': betweenness_centrality,
                'closeness': closeness_centrality
            }
            
            # ê°€ì¥ ì¤‘ìš”í•œ ë…¸ë“œë“¤
            metrics['top_nodes'] = {
                'by_degree': sorted(degree_centrality.items(), 
                                  key=lambda x: x[1], reverse=True)[:5],
                'by_betweenness': sorted(betweenness_centrality.items(), 
                                       key=lambda x: x[1], reverse=True)[:5]
            }
            
            # ì—°ê²° ì„±ë¶„
            if nx.is_connected(G):
                metrics['diameter'] = nx.diameter(G)
                metrics['average_path_length'] = nx.average_shortest_path_length(G)
            else:
                components = list(nx.connected_components(G))
                metrics['connected_components'] = len(components)
                metrics['largest_component_size'] = len(max(components, key=len))
            
        except Exception as e:
            self.logger.warning(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            metrics['error'] = str(e)
        
        return metrics
    
    def analyze_sns_comments_network(self, comments_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """SNS ëŒ“ê¸€ ë°ì´í„°ì—ì„œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        self.logger.info("ğŸ’¬ SNS ëŒ“ê¸€ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹œì‘")
        
        # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = []
        for comment in comments_data:
            text = comment.get('text', '') or comment.get('content', '') or comment.get('body', '')
            if text and len(text.strip()) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                texts.append(text.strip())
        
        if not texts:
            self.logger.warning("ë¶„ì„í•  ëŒ“ê¸€ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {'error': 'No valid comment texts found'}
        
        # ê°œë… ê´€ê³„ ë¶„ì„
        network_result = self.analyze_concept_relationships(texts)
        
        # ì¶”ê°€ ë¶„ì„: ì‹œê°„ë³„ íŠ¸ë Œë“œ (ëŒ“ê¸€ì— ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        if comments_data and 'timestamp' in comments_data[0]:
            network_result['temporal_analysis'] = self._analyze_temporal_trends(comments_data)
        
        return network_result
    
    def _analyze_temporal_trends(self, comments_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì‹œê°„ë³„ ê°œë… íŠ¸ë Œë“œ ë¶„ì„"""
        # ì‹œê°„ë³„ë¡œ ëŒ“ê¸€ ê·¸ë£¹í™”
        from collections import defaultdict
        import datetime
        
        time_groups = defaultdict(list)
        
        for comment in comments_data:
            timestamp = comment.get('timestamp')
            if timestamp:
                # ì‹œê°„ì„ ì‹œê°„ëŒ€ë³„ë¡œ ê·¸ë£¹í™” (ì˜ˆ: 1ì‹œê°„ ë‹¨ìœ„)
                if isinstance(timestamp, str):
                    try:
                        dt = datetime.datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    except:
                        continue
                else:
                    dt = timestamp
                
                hour_key = dt.replace(minute=0, second=0, microsecond=0)
                time_groups[hour_key].append(comment.get('text', ''))
        
        # ì‹œê°„ëŒ€ë³„ ê°œë… ë¶„ì„
        temporal_trends = {}
        for time_key, texts in time_groups.items():
            if texts:
                concepts, _ = self.extract_economic_concepts(' '.join(texts))
                temporal_trends[time_key.isoformat()] = {
                    concept: data['score'] for concept, data in concepts.items()
                }
        
        return temporal_trends
    
    def generate_network_insights(self, network_result: Dict[str, Any]) -> List[str]:
        """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼ì—ì„œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        if 'error' in network_result:
            return ["ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]
        
        G = network_result.get('graph')
        metrics = network_result.get('metrics', {})
        concepts = network_result.get('concepts', {})
        
        if not G or len(G.nodes()) == 0:
            return ["ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²½ì œ ê°œë…ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."]
        
        # 1. ì „ì²´ ë„¤íŠ¸ì›Œí¬ ê·œëª¨
        insights.append(f"ğŸ“Š ì´ {len(G.nodes())}ê°œì˜ ê²½ì œ ê°œë…ê³¼ {len(G.edges())}ê°œì˜ ê´€ê³„ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        # 2. ê°€ì¥ ì¤‘ìš”í•œ ê°œë…ë“¤
        if 'centrality' in metrics and 'degree' in metrics['centrality']:
            top_concepts = sorted(metrics['centrality']['degree'].items(), 
                                key=lambda x: x[1], reverse=True)[:3]
            concept_names = [self._get_concept_display_name(concept) for concept, _ in top_concepts]
            insights.append(f"ğŸ¯ ê°€ì¥ ì¤‘ìš”í•œ ê²½ì œ ê°œë…: {', '.join(concept_names)}")
        
        # 3. ë„¤íŠ¸ì›Œí¬ ë°€ë„
        if 'density' in metrics:
            density = metrics['density']
            if density > 0.3:
                insights.append("ğŸ”— ê²½ì œ ê°œë…ë“¤ ê°„ì˜ ì—°ê²°ì´ ë§¤ìš° ë°€ì ‘í•©ë‹ˆë‹¤.")
            elif density > 0.1:
                insights.append("ğŸ”— ê²½ì œ ê°œë…ë“¤ ê°„ì˜ ì—°ê²°ì´ ì ë‹¹í•©ë‹ˆë‹¤.")
            else:
                insights.append("ğŸ”— ê²½ì œ ê°œë…ë“¤ ê°„ì˜ ì—°ê²°ì´ ëŠìŠ¨í•©ë‹ˆë‹¤.")
        
        # 4. ê°ì • ë¶„ì„ ê²°ê³¼
        concept_sentiments = network_result.get('concept_sentiments', {})
        if concept_sentiments:
            positive_concepts = []
            negative_concepts = []
            
            for concept, sentiments in concept_sentiments.items():
                avg_sentiment = np.mean(sentiments) if sentiments else 0
                if avg_sentiment > 0.1:
                    positive_concepts.append(self._get_concept_display_name(concept))
                elif avg_sentiment < -0.1:
                    negative_concepts.append(self._get_concept_display_name(concept))
            
            if positive_concepts:
                insights.append(f"ğŸ˜Š ê¸ì •ì  ì–¸ê¸‰: {', '.join(positive_concepts[:3])}")
            if negative_concepts:
                insights.append(f"ğŸ˜Ÿ ë¶€ì •ì  ì–¸ê¸‰: {', '.join(negative_concepts[:3])}")
        
        # 5. íŠ¹ë³„í•œ ê´€ê³„ íŒ¨í„´
        strong_relationships = []
        for edge in G.edges(data=True):
            if edge[2].get('weight', 0) > 0.7:
                concept1_name = self._get_concept_display_name(edge[0])
                concept2_name = self._get_concept_display_name(edge[1])
                relationship_type = edge[2].get('relationship_type', 'related')
                strong_relationships.append(f"{concept1_name} â†” {concept2_name}")
        
        if strong_relationships:
            insights.append(f"ğŸ”¥ ê°•í•œ ì—°ê´€ì„±: {', '.join(strong_relationships[:2])}")
        
        return insights
    
    def _get_concept_display_name(self, concept_key: str) -> str:
        """ê°œë… í‚¤ë¥¼ í‘œì‹œìš© ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        display_names = {
            'monetary_policy': 'í†µí™”ì •ì±…',
            'inflation': 'ì¸í”Œë ˆì´ì…˜',
            'stock_market': 'ì£¼ì‹ì‹œì¥',
            'corporate_performance': 'ê¸°ì—…ì‹¤ì ',
            'technology': 'ê¸°ìˆ ì£¼',
            'financial_sector': 'ê¸ˆìœµì„¹í„°',
            'energy': 'ì—ë„ˆì§€',
            'real_estate': 'ë¶€ë™ì‚°',
            'international_trade': 'êµ­ì œë¬´ì—­',
            'cryptocurrency': 'ì•”í˜¸í™”í',
            'esg': 'ESG',
            'labor_market': 'ê³ ìš©ì‹œì¥',
            'consumer_spending': 'ì†Œë¹„',
            'government_policy': 'ì •ë¶€ì •ì±…',
            'geopolitical_risk': 'ì§€ì •í•™ì  ë¦¬ìŠ¤í¬',
            'market_sentiment': 'ì‹œì¥ì‹¬ë¦¬'
        }
        return display_names.get(concept_key, concept_key.replace('_', ' ').title())
