#!/usr/bin/env python3
"""
ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ëª¨ë“ˆ
ë‰´ìŠ¤ ë° SNS ë°ì´í„°ë¥¼ í™œìš©í•œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
"""

import networkx as nx
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter, defaultdict
import re
from textblob import TextBlob

class SocialNetworkAnalyzer:
    """ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        
        # ê²½ì œ ê´€ë ¨ ì—”í‹°í‹° (ê¸°ì—…, ì¸ë¬¼, ê¸°ê´€ ë“±) - ë” í¬ê´„ì ìœ¼ë¡œ í™•ì¥
        self.economic_entities = {
            # ì£¼ìš” ê¸°ì—… (ë” ë§ì€ ê¸°ì—… ì¶”ê°€)
            'companies': [
                'Apple', 'Microsoft', 'Google', 'Alphabet', 'Amazon', 'Tesla', 'Meta', 'Netflix',
                'Samsung', 'TSMC', 'ASML', 'Nvidia', 'Intel', 'AMD', 'Qualcomm',
                'JPMorgan', 'Goldman Sachs', 'Morgan Stanley', 'Bank of America', 'Wells Fargo',
                'Berkshire Hathaway', 'Visa', 'Mastercard', 'PayPal', 'Square',
                'Coca-Cola', 'PepsiCo', 'Johnson & Johnson', 'Pfizer', 'Moderna',
                'Walmart', 'Target', 'Home Depot', 'McDonald\'s', 'Starbucks',
                'Boeing', 'Airbus', 'General Electric', 'Ford', 'GM', 'Toyota',
                'ExxonMobil', 'Chevron', 'Shell', 'BP'
            ],
            
            # ì£¼ìš” ì¸ë¬¼ (ë” ë§ì€ ì¸ë¬¼ ì¶”ê°€)
            'people': [
                'Jerome Powell', 'Janet Yellen', 'Christine Lagarde', 'Jay Powell',
                'Elon Musk', 'Warren Buffett', 'Jeff Bezos', 'Bill Gates', 'Tim Cook', 
                'Satya Nadella', 'Mark Zuckerberg', 'Larry Page', 'Sergey Brin',
                'Jamie Dimon', 'Larry Fink', 'Ray Dalio', 'Cathie Wood',
                'Charlie Munger', 'Michael Burry', 'Carl Icahn'
            ],
            
            # ê¸°ê´€ ë° ì •ë¶€ (ë” í¬ê´„ì )
            'institutions': [
                'Federal Reserve', 'Fed', 'Treasury', 'SEC', 'FDIC', 'CFTC',
                'ECB', 'European Central Bank', 'Bank of Japan', 'BOJ',
                'People\'s Bank of China', 'PBOC', 'Bank of England', 'BOE',
                'IMF', 'World Bank', 'OECD', 'G7', 'G20', 'WTO',
                'Congress', 'Senate', 'House', 'White House', 'Biden Administration',
                'Supreme Court', 'Justice Department'
            ],
            
            # ê²½ì œ ê°œë… (ë” ìƒì„¸í•˜ê²Œ)
            'concepts': [
                'inflation', 'deflation', 'recession', 'depression', 'recovery',
                'GDP', 'unemployment', 'interest rates', 'interest rate', 'rates',
                'quantitative easing', 'QE', 'monetary policy', 'fiscal policy',
                'cryptocurrency', 'crypto', 'Bitcoin', 'Ethereum', 'blockchain',
                'ESG', 'climate change', 'supply chain', 'trade war', 'tariffs',
                'stock market', 'bond market', 'commodities', 'oil prices',
                'dollar', 'euro', 'yen', 'yuan', 'currency', 'forex',
                'earnings', 'revenue', 'profit', 'loss', 'merger', 'acquisition',
                'IPO', 'dividend', 'buyback', 'split', 'volatility'
            ]
        }
        
        # ê´€ê³„ í‚¤ì›Œë“œ
        self.relationship_keywords = {
            'partnership': ['partnership', 'collaboration', 'joint venture', 'alliance'],
            'competition': ['compete', 'rival', 'challenge', 'threat'],
            'investment': ['invest', 'funding', 'acquisition', 'merger', 'buyout'],
            'regulation': ['regulate', 'policy', 'rule', 'compliance', 'oversight'],
            'influence': ['impact', 'affect', 'influence', 'drive', 'cause'],
            'criticism': ['criticize', 'oppose', 'against', 'dispute', 'conflict']
        }
        
        self.logger.info("âœ… ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_entities_from_text(self, text: str) -> Dict[str, List[str]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê²½ì œ ê´€ë ¨ ì—”í‹°í‹° ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        text_lower = text.lower()
        # HTML íƒœê·¸ ì œê±°
        import re
        text_clean = re.sub(r'<[^>]+>', '', text)
        text_clean_lower = text_clean.lower()
        
        found_entities = {
            'companies': [],
            'people': [],
            'institutions': [],
            'concepts': []
        }
        
        for category, entities in self.economic_entities.items():
            for entity in entities:
                entity_lower = entity.lower()
                
                # ì •í™•í•œ ë§¤ì¹­ê³¼ ë¶€ë¶„ ë§¤ì¹­ ëª¨ë‘ ê³ ë ¤
                if (entity_lower in text_clean_lower or 
                    any(word in text_clean_lower for word in entity_lower.split() if len(word) > 2)):
                    
                    # ì¤‘ë³µ ì œê±°
                    if entity not in found_entities[category]:
                        found_entities[category].append(entity)
        
        # ì¶”ê°€ì ì¸ íŒ¨í„´ ë§¤ì¹­ (ì•½ì–´, ë³€í˜• ë“±)
        additional_patterns = {
            'institutions': {
                'federal reserve': ['fed', 'federal reserve', 'central bank'],
                'sec': ['securities and exchange commission', 'sec'],
                'treasury': ['treasury department', 'treasury', 'us treasury'],
                'ecb': ['european central bank', 'ecb']
            },
            'concepts': {
                'interest rates': ['interest rate', 'rates', 'fed rate', 'federal funds rate'],
                'inflation': ['inflation', 'cpi', 'consumer price index', 'price increases'],
                'recession': ['recession', 'economic downturn', 'contraction'],
                'stock market': ['stock market', 'equity market', 'stocks', 'shares']
            }
        }
        
        for category, pattern_dict in additional_patterns.items():
            for main_entity, patterns in pattern_dict.items():
                for pattern in patterns:
                    if pattern in text_clean_lower:
                        # ë©”ì¸ ì—”í‹°í‹° ì´ë¦„ìœ¼ë¡œ ì •ê·œí™”
                        if main_entity not in [e.lower() for e in found_entities[category]]:
                            found_entities[category].append(main_entity.title())
        
        return found_entities
    
    def extract_relationships_from_text(self, text: str, entities: Dict[str, List[str]]) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ê°„ ê´€ê³„ ì¶”ì¶œ"""
        relationships = []
        text_lower = text.lower()
        
        # ëª¨ë“  ì—”í‹°í‹°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
        all_entities = []
        for category, entity_list in entities.items():
            for entity in entity_list:
                all_entities.append({'name': entity, 'category': category})
        
        # ì—”í‹°í‹° ìŒì— ëŒ€í•´ ê´€ê³„ ê²€ìƒ‰
        for i, entity1 in enumerate(all_entities):
            for j, entity2 in enumerate(all_entities[i+1:], i+1):
                # ë‘ ì—”í‹°í‹°ê°€ ê°™ì€ ë¬¸ì¥ì— ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
                sentences = text.split('.')
                
                for sentence in sentences:
                    sentence_lower = sentence.lower()
                    
                    if (entity1['name'].lower() in sentence_lower and 
                        entity2['name'].lower() in sentence_lower):
                        
                        # ê´€ê³„ ìœ í˜• ê²°ì •
                        relationship_type = self._determine_relationship_type(sentence_lower)
                        
                        relationships.append({
                            'source': entity1['name'],
                            'target': entity2['name'],
                            'source_category': entity1['category'],
                            'target_category': entity2['category'],
                            'relationship_type': relationship_type,
                            'context': sentence.strip(),
                            'weight': 1
                        })
        
        return relationships
    
    def _determine_relationship_type(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ê´€ê³„ ìœ í˜• ê²°ì •"""
        for rel_type, keywords in self.relationship_keywords.items():
            if any(keyword in text for keyword in keywords):
                return rel_type
        return 'mentioned_together'
    
    def analyze_news_network(self, news_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        self.logger.info("ğŸ“° ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹œì‘")
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        G = nx.Graph()
        
        # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ì²˜ë¦¬
        categories = news_data.get('categories', {})
        all_relationships = []
        entity_mentions = defaultdict(int)
        entity_sentiments = defaultdict(list)
        
        for category, articles in categories.items():
            for article in articles:
                title = article.get('title', '')
                summary = article.get('summary', '')
                sentiment = article.get('sentiment', {})
                
                # ì „ì²´ í…ìŠ¤íŠ¸
                full_text = f"{title} {summary}"
                
                # ì—”í‹°í‹° ì¶”ì¶œ
                entities = self.extract_entities_from_text(full_text)
                
                # ì—”í‹°í‹° ì–¸ê¸‰ íšŸìˆ˜ ë° ê°ì • ê¸°ë¡
                for category_entities in entities.values():
                    for entity in category_entities:
                        entity_mentions[entity] += 1
                        entity_sentiments[entity].append(sentiment.get('polarity', 0))
                
                # ê´€ê³„ ì¶”ì¶œ
                relationships = self.extract_relationships_from_text(full_text, entities)
                all_relationships.extend(relationships)
        
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ êµ¬ì„±
        for rel in all_relationships:
            source = rel['source']
            target = rel['target']
            
            if G.has_edge(source, target):
                G[source][target]['weight'] += rel['weight']
                G[source][target]['contexts'].append(rel['context'])
            else:
                G.add_edge(source, target, 
                          weight=rel['weight'],
                          relationship_type=rel['relationship_type'],
                          contexts=[rel['context']])
        
        # ë…¸ë“œ ì†ì„± ì¶”ê°€
        for node in G.nodes():
            G.nodes[node]['mentions'] = entity_mentions.get(node, 0)
            sentiments = entity_sentiments.get(node, [0])
            G.nodes[node]['avg_sentiment'] = np.mean(sentiments)
            G.nodes[node]['sentiment_std'] = np.std(sentiments)
        
        # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ë©”íŠ¸ë¦­ ê³„ì‚°
        network_metrics = self._calculate_network_metrics(G)
        
        # ì»¤ë®¤ë‹ˆí‹° íƒì§€
        communities = self._detect_communities(G)
        
        # ì¤‘ìš”í•œ ë…¸ë“œ ì‹ë³„
        important_nodes = self._identify_important_nodes(G)
        
        return {
            'graph': G,
            'network_metrics': network_metrics,
            'communities': communities,
            'important_nodes': important_nodes,
            'total_entities': len(G.nodes()),
            'total_relationships': len(G.edges()),
            'entity_mentions': dict(entity_mentions),
            'entity_sentiments': {k: np.mean(v) for k, v in entity_sentiments.items()},
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def analyze_reddit_network(self, reddit_data: Dict[str, Any]) -> Dict[str, Any]:
        """Reddit ë°ì´í„°ì—ì„œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        self.logger.info("ğŸ“± Reddit ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì‹œì‘")
        
        # ì‚¬ìš©ì ê°„ ìƒí˜¸ì‘ìš© ë„¤íŠ¸ì›Œí¬
        user_network = nx.Graph()
        
        # ì£¼ì œ ê°„ ì—°ê´€ì„± ë„¤íŠ¸ì›Œí¬
        topic_network = nx.Graph()
        
        # Reddit ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ë¶„ì„
        # (ì‹¤ì œ Reddit ë°ì´í„°ê°€ ìˆë‹¤ë©´ í•´ë‹¹ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
        
        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ì˜ˆì‹œ ë„¤íŠ¸ì›Œí¬ ìƒì„±
        sample_users = ['user1', 'user2', 'user3', 'user4', 'user5']
        sample_topics = ['inflation', 'fed_policy', 'stock_market', 'crypto', 'recession']
        
        # ì‚¬ìš©ì ê°„ ìƒí˜¸ì‘ìš© (ëŒ“ê¸€, ë‹µê¸€ ë“±)
        for i, user1 in enumerate(sample_users):
            for user2 in sample_users[i+1:]:
                if np.random.random() > 0.6:  # 40% í™•ë¥ ë¡œ ìƒí˜¸ì‘ìš©
                    interaction_strength = np.random.randint(1, 10)
                    user_network.add_edge(user1, user2, weight=interaction_strength)
        
        # ì£¼ì œ ê°„ ì—°ê´€ì„±
        for i, topic1 in enumerate(sample_topics):
            for topic2 in sample_topics[i+1:]:
                if np.random.random() > 0.5:  # 50% í™•ë¥ ë¡œ ì—°ê´€ì„±
                    correlation = np.random.random()
                    topic_network.add_edge(topic1, topic2, weight=correlation)
        
        # ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        user_metrics = self._calculate_network_metrics(user_network)
        topic_metrics = self._calculate_network_metrics(topic_network)
        
        return {
            'user_network': user_network,
            'topic_network': topic_network,
            'user_metrics': user_metrics,
            'topic_metrics': topic_metrics,
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def _calculate_network_metrics(self, G: nx.Graph) -> Dict[str, Any]:
        """ë„¤íŠ¸ì›Œí¬ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if len(G.nodes()) == 0:
            return {}
        
        metrics = {
            'nodes_count': len(G.nodes()),
            'edges_count': len(G.edges()),
            'density': nx.density(G),
            'average_clustering': nx.average_clustering(G),
        }
        
        if len(G.nodes()) > 0:
            # ì¤‘ì‹¬ì„± ì¸¡ì •
            degree_centrality = nx.degree_centrality(G)
            betweenness_centrality = nx.betweenness_centrality(G)
            closeness_centrality = nx.closeness_centrality(G)
            
            metrics.update({
                'top_degree_centrality': sorted(degree_centrality.items(), 
                                               key=lambda x: x[1], reverse=True)[:5],
                'top_betweenness_centrality': sorted(betweenness_centrality.items(), 
                                                   key=lambda x: x[1], reverse=True)[:5],
                'top_closeness_centrality': sorted(closeness_centrality.items(), 
                                                 key=lambda x: x[1], reverse=True)[:5]
            })
        
        # ì—°ê²°ì„± ë¶„ì„
        if nx.is_connected(G):
            metrics['diameter'] = nx.diameter(G)
            metrics['average_path_length'] = nx.average_shortest_path_length(G)
        else:
            metrics['connected_components'] = nx.number_connected_components(G)
            largest_cc = max(nx.connected_components(G), key=len)
            metrics['largest_component_size'] = len(largest_cc)
        
        return metrics
    
    def _detect_communities(self, G: nx.Graph) -> List[List[str]]:
        """ì»¤ë®¤ë‹ˆí‹° íƒì§€"""
        try:
            # Louvain ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© (community íŒ¨í‚¤ì§€ í•„ìš”)
            import community as community_louvain
            partition = community_louvain.best_partition(G)
            
            # ì»¤ë®¤ë‹ˆí‹°ë³„ë¡œ ë…¸ë“œ ê·¸ë£¹í™”
            communities = defaultdict(list)
            for node, community_id in partition.items():
                communities[community_id].append(node)
            
            return list(communities.values())
        
        except ImportError:
            # community íŒ¨í‚¤ì§€ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì—°ê²° ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
            return [list(component) for component in nx.connected_components(G)]
    
    def _identify_important_nodes(self, G: nx.Graph) -> Dict[str, List[Tuple[str, float]]]:
        """ì¤‘ìš”í•œ ë…¸ë“œ ì‹ë³„"""
        if len(G.nodes()) == 0:
            return {}
        
        # ë‹¤ì–‘í•œ ì¤‘ì‹¬ì„± ì¸¡ì •
        degree_centrality = nx.degree_centrality(G)
        betweenness_centrality = nx.betweenness_centrality(G)
        
        # ì–¸ê¸‰ íšŸìˆ˜ ê¸°ë°˜ ì¤‘ìš”ë„
        mention_importance = {}
        for node in G.nodes():
            mentions = G.nodes[node].get('mentions', 0)
            mention_importance[node] = mentions
        
        # ê°ì • ê¸°ë°˜ ì¤‘ìš”ë„ (ì ˆëŒ“ê°’ì´ í´ìˆ˜ë¡ ì¤‘ìš”)
        sentiment_importance = {}
        for node in G.nodes():
            sentiment = abs(G.nodes[node].get('avg_sentiment', 0))
            sentiment_importance[node] = sentiment
        
        return {
            'by_degree': sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10],
            'by_betweenness': sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10],
            'by_mentions': sorted(mention_importance.items(), key=lambda x: x[1], reverse=True)[:10],
            'by_sentiment_impact': sorted(sentiment_importance.items(), key=lambda x: x[1], reverse=True)[:10]
        }
    
    def generate_network_insights(self, news_network: Dict[str, Any], 
                                reddit_network: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {
            'timestamp': datetime.now().isoformat(),
            'news_insights': {},
            'reddit_insights': {},
            'combined_insights': {}
        }
        
        # ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬ ì¸ì‚¬ì´íŠ¸
        if news_network:
            G = news_network['graph']
            metrics = news_network['network_metrics']
            important_nodes = news_network['important_nodes']
            
            insights['news_insights'] = {
                'network_size': f"{metrics.get('nodes_count', 0)}ê°œ ì—”í‹°í‹°, {metrics.get('edges_count', 0)}ê°œ ê´€ê³„",
                'network_density': f"{metrics.get('density', 0):.3f} (0-1 ë²”ìœ„)",
                'clustering': f"{metrics.get('average_clustering', 0):.3f}",
                'most_central_entities': [node for node, _ in important_nodes.get('by_degree', [])[:3]],
                'most_mentioned_entities': [node for node, _ in important_nodes.get('by_mentions', [])[:3]],
                'sentiment_leaders': [node for node, _ in important_nodes.get('by_sentiment_impact', [])[:3]],
                'communities_count': len(news_network.get('communities', [])),
                'key_relationships': self._extract_key_relationships(G)
            }
        
        # Reddit ë„¤íŠ¸ì›Œí¬ ì¸ì‚¬ì´íŠ¸ (êµ¬í˜„ëœ ê²½ìš°)
        if reddit_network:
            insights['reddit_insights'] = {
                'user_network_size': reddit_network['user_metrics'].get('nodes_count', 0),
                'topic_network_size': reddit_network['topic_metrics'].get('nodes_count', 0),
                'user_interactions': reddit_network['user_metrics'].get('edges_count', 0),
                'topic_correlations': reddit_network['topic_metrics'].get('edges_count', 0)
            }
        
        # í†µí•© ì¸ì‚¬ì´íŠ¸
        insights['combined_insights'] = {
            'analysis_summary': f"ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬ì—ì„œ {news_network.get('total_entities', 0)}ê°œ ì—”í‹°í‹° ë¶„ì„",
            'key_findings': self._generate_key_findings(news_network),
            'recommendations': self._generate_recommendations(news_network)
        }
        
        return insights
    
    def _extract_key_relationships(self, G: nx.Graph) -> List[Dict[str, Any]]:
        """ì£¼ìš” ê´€ê³„ ì¶”ì¶œ"""
        key_relationships = []
        
        # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ê´€ê³„ë“¤
        edges_by_weight = sorted(G.edges(data=True), 
                               key=lambda x: x[2].get('weight', 0), reverse=True)
        
        for source, target, data in edges_by_weight[:5]:
            key_relationships.append({
                'source': source,
                'target': target,
                'strength': data.get('weight', 0),
                'type': data.get('relationship_type', 'unknown'),
                'context_sample': data.get('contexts', [''])[0][:100] + "..."
            })
        
        return key_relationships
    
    def _generate_key_findings(self, news_network: Dict[str, Any]) -> List[str]:
        """ì£¼ìš” ë°œê²¬ì‚¬í•­ ìƒì„±"""
        findings = []
        
        important_nodes = news_network.get('important_nodes', {})
        metrics = news_network.get('network_metrics', {})
        
        # ê°€ì¥ ì¤‘ì‹¬ì ì¸ ì—”í‹°í‹°
        if important_nodes.get('by_degree'):
            top_entity = important_nodes['by_degree'][0][0]
            findings.append(f"'{top_entity}'ê°€ ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬ì—ì„œ ê°€ì¥ ì¤‘ì‹¬ì ì¸ ì—­í• ì„ í•¨")
        
        # ë„¤íŠ¸ì›Œí¬ ë°€ë„
        density = metrics.get('density', 0)
        if density > 0.3:
            findings.append("ë†’ì€ ë„¤íŠ¸ì›Œí¬ ë°€ë„ë¡œ ì—”í‹°í‹° ê°„ ê°•í•œ ì—°ê²°ì„±ì„ ë³´ì„")
        elif density < 0.1:
            findings.append("ë‚®ì€ ë„¤íŠ¸ì›Œí¬ ë°€ë„ë¡œ ì—”í‹°í‹° ê°„ ì—°ê²°ì´ ì œí•œì ì„")
        
        # ì»¤ë®¤ë‹ˆí‹° ìˆ˜
        communities_count = len(news_network.get('communities', []))
        if communities_count > 1:
            findings.append(f"{communities_count}ê°œì˜ ì£¼ìš” ì—”í‹°í‹° ê·¸ë£¹ì´ ì‹ë³„ë¨")
        
        return findings
    
    def _generate_recommendations(self, news_network: Dict[str, Any]) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        important_nodes = news_network.get('important_nodes', {})
        
        # ëª¨ë‹ˆí„°ë§ ì¶”ì²œ
        if important_nodes.get('by_mentions'):
            top_mentioned = important_nodes['by_mentions'][:3]
            entities = [node for node, _ in top_mentioned]
            recommendations.append(f"ë‹¤ìŒ ì—”í‹°í‹°ë“¤ì„ ì¤‘ì  ëª¨ë‹ˆí„°ë§: {', '.join(entities)}")
        
        # ê°ì • ë¶„ì„ ì¶”ì²œ
        if important_nodes.get('by_sentiment_impact'):
            sentiment_leaders = important_nodes['by_sentiment_impact'][:2]
            entities = [node for node, _ in sentiment_leaders]
            recommendations.append(f"ê°ì • ë³€í™” ì£¼ì˜ ê¹Šê²Œ ê´€ì°°: {', '.join(entities)}")
        
        # ë„¤íŠ¸ì›Œí¬ í™•ì¥ ì¶”ì²œ
        recommendations.append("ê´€ê³„ ë„¤íŠ¸ì›Œí¬ í™•ì¥ì„ ìœ„í•´ ë” ë§ì€ ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ê°€ ê³ ë ¤")
        
        return recommendations

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ•¸ï¸ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    analyzer = SocialNetworkAnalyzer()
    
    # ìƒ˜í”Œ ë‰´ìŠ¤ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    sample_news_data = {
        'categories': {
            'financial': [
                {
                    'title': 'Federal Reserve raises interest rates amid inflation concerns',
                    'summary': 'Jerome Powell announced the Fed decision to combat rising inflation',
                    'sentiment': {'polarity': -0.2}
                },
                {
                    'title': 'Apple and Microsoft partnership on AI technology',
                    'summary': 'Tech giants collaborate on artificial intelligence development',
                    'sentiment': {'polarity': 0.3}
                }
            ]
        }
    }
    
    # ë‰´ìŠ¤ ë„¤íŠ¸ì›Œí¬ ë¶„ì„
    news_network = analyzer.analyze_news_network(sample_news_data)
    
    print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"  ì—”í‹°í‹° ìˆ˜: {news_network['total_entities']}ê°œ")
    print(f"  ê´€ê³„ ìˆ˜: {news_network['total_relationships']}ê°œ")
    
    # ì¸ì‚¬ì´íŠ¸ ìƒì„±
    insights = analyzer.generate_network_insights(news_network)
    
    print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    for finding in insights['combined_insights']['key_findings']:
        print(f"  â€¢ {finding}")
    
    print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
    for recommendation in insights['combined_insights']['recommendations']:
        print(f"  â€¢ {recommendation}")

if __name__ == "__main__":
    main()
